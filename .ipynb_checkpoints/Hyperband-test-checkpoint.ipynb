{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('/home/javier.mas/hyperband/hyperband')\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from math import log\n",
    "from sklearn.metrics import log_loss, cohen_kappa_score, confusion_matrix\n",
    "from hyperband.hyperband import Hyperband\n",
    "from hyperband.defs.xgb import get_params, try_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\t   Hyperband-test.ipynb  notebooks  Untitled.ipynb\r\n",
      "hyperband  __init__.py\t\t tiago\r\n"
     ]
    }
   ],
   "source": [
    "!ls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/HR_comma_sep.csv')\n",
    "data = pd.get_dummies(data)\n",
    "data_dict = dict()\n",
    "feats = [col for col in data.columns if col != 'left']\n",
    "data_dict['x_train'], data_dict['y_train'] = data.loc[int(len(data)*0.8):, feats], data.loc[int(len(data)*0.8):, 'left']\n",
    "data_dict['x_test'], data_dict['y_test'] = data.loc[:int(len(data)*0.8), feats], data.loc[:int(len(data)*0.8), 'left']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>satisfaction_level</th>\n",
       "      <th>last_evaluation</th>\n",
       "      <th>number_project</th>\n",
       "      <th>average_montly_hours</th>\n",
       "      <th>time_spend_company</th>\n",
       "      <th>Work_accident</th>\n",
       "      <th>promotion_last_5years</th>\n",
       "      <th>sales_IT</th>\n",
       "      <th>sales_RandD</th>\n",
       "      <th>sales_accounting</th>\n",
       "      <th>sales_hr</th>\n",
       "      <th>sales_management</th>\n",
       "      <th>sales_marketing</th>\n",
       "      <th>sales_product_mng</th>\n",
       "      <th>sales_sales</th>\n",
       "      <th>sales_support</th>\n",
       "      <th>sales_technical</th>\n",
       "      <th>salary_high</th>\n",
       "      <th>salary_low</th>\n",
       "      <th>salary_medium</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.53</td>\n",
       "      <td>2</td>\n",
       "      <td>157</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5</td>\n",
       "      <td>262</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.11</td>\n",
       "      <td>0.88</td>\n",
       "      <td>7</td>\n",
       "      <td>272</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0.87</td>\n",
       "      <td>5</td>\n",
       "      <td>223</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.37</td>\n",
       "      <td>0.52</td>\n",
       "      <td>2</td>\n",
       "      <td>159</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.41</td>\n",
       "      <td>0.50</td>\n",
       "      <td>2</td>\n",
       "      <td>153</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.77</td>\n",
       "      <td>6</td>\n",
       "      <td>247</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.92</td>\n",
       "      <td>0.85</td>\n",
       "      <td>5</td>\n",
       "      <td>259</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.89</td>\n",
       "      <td>1.00</td>\n",
       "      <td>5</td>\n",
       "      <td>224</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.42</td>\n",
       "      <td>0.53</td>\n",
       "      <td>2</td>\n",
       "      <td>142</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.45</td>\n",
       "      <td>0.54</td>\n",
       "      <td>2</td>\n",
       "      <td>135</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.11</td>\n",
       "      <td>0.81</td>\n",
       "      <td>6</td>\n",
       "      <td>305</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.92</td>\n",
       "      <td>4</td>\n",
       "      <td>234</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.41</td>\n",
       "      <td>0.55</td>\n",
       "      <td>2</td>\n",
       "      <td>148</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.36</td>\n",
       "      <td>0.56</td>\n",
       "      <td>2</td>\n",
       "      <td>137</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.54</td>\n",
       "      <td>2</td>\n",
       "      <td>143</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.45</td>\n",
       "      <td>0.47</td>\n",
       "      <td>2</td>\n",
       "      <td>160</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.78</td>\n",
       "      <td>0.99</td>\n",
       "      <td>4</td>\n",
       "      <td>255</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.45</td>\n",
       "      <td>0.51</td>\n",
       "      <td>2</td>\n",
       "      <td>160</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.76</td>\n",
       "      <td>0.89</td>\n",
       "      <td>5</td>\n",
       "      <td>262</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.11</td>\n",
       "      <td>0.83</td>\n",
       "      <td>6</td>\n",
       "      <td>282</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.55</td>\n",
       "      <td>2</td>\n",
       "      <td>147</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.09</td>\n",
       "      <td>0.95</td>\n",
       "      <td>6</td>\n",
       "      <td>304</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.46</td>\n",
       "      <td>0.57</td>\n",
       "      <td>2</td>\n",
       "      <td>139</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.53</td>\n",
       "      <td>2</td>\n",
       "      <td>158</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.92</td>\n",
       "      <td>5</td>\n",
       "      <td>242</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.82</td>\n",
       "      <td>0.87</td>\n",
       "      <td>4</td>\n",
       "      <td>239</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.49</td>\n",
       "      <td>2</td>\n",
       "      <td>135</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.41</td>\n",
       "      <td>0.46</td>\n",
       "      <td>2</td>\n",
       "      <td>128</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.50</td>\n",
       "      <td>2</td>\n",
       "      <td>132</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11970</th>\n",
       "      <td>0.14</td>\n",
       "      <td>0.38</td>\n",
       "      <td>5</td>\n",
       "      <td>115</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11971</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.89</td>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11972</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.81</td>\n",
       "      <td>3</td>\n",
       "      <td>239</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11973</th>\n",
       "      <td>0.49</td>\n",
       "      <td>0.71</td>\n",
       "      <td>4</td>\n",
       "      <td>178</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11974</th>\n",
       "      <td>0.82</td>\n",
       "      <td>0.58</td>\n",
       "      <td>5</td>\n",
       "      <td>263</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11975</th>\n",
       "      <td>0.59</td>\n",
       "      <td>0.77</td>\n",
       "      <td>3</td>\n",
       "      <td>272</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11976</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.82</td>\n",
       "      <td>3</td>\n",
       "      <td>133</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11977</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.72</td>\n",
       "      <td>3</td>\n",
       "      <td>149</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11978</th>\n",
       "      <td>0.61</td>\n",
       "      <td>0.68</td>\n",
       "      <td>3</td>\n",
       "      <td>193</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11979</th>\n",
       "      <td>0.52</td>\n",
       "      <td>0.55</td>\n",
       "      <td>5</td>\n",
       "      <td>174</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11980</th>\n",
       "      <td>0.79</td>\n",
       "      <td>0.87</td>\n",
       "      <td>4</td>\n",
       "      <td>223</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11981</th>\n",
       "      <td>0.49</td>\n",
       "      <td>0.89</td>\n",
       "      <td>4</td>\n",
       "      <td>201</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11982</th>\n",
       "      <td>0.73</td>\n",
       "      <td>0.67</td>\n",
       "      <td>2</td>\n",
       "      <td>139</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11983</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.49</td>\n",
       "      <td>5</td>\n",
       "      <td>241</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11984</th>\n",
       "      <td>0.52</td>\n",
       "      <td>0.61</td>\n",
       "      <td>4</td>\n",
       "      <td>187</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11985</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0.64</td>\n",
       "      <td>4</td>\n",
       "      <td>192</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11986</th>\n",
       "      <td>0.48</td>\n",
       "      <td>0.50</td>\n",
       "      <td>5</td>\n",
       "      <td>142</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11987</th>\n",
       "      <td>0.19</td>\n",
       "      <td>0.79</td>\n",
       "      <td>4</td>\n",
       "      <td>229</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11988</th>\n",
       "      <td>0.49</td>\n",
       "      <td>0.49</td>\n",
       "      <td>3</td>\n",
       "      <td>104</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11989</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.76</td>\n",
       "      <td>3</td>\n",
       "      <td>255</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11990</th>\n",
       "      <td>0.49</td>\n",
       "      <td>0.49</td>\n",
       "      <td>4</td>\n",
       "      <td>212</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11991</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0.53</td>\n",
       "      <td>2</td>\n",
       "      <td>235</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11992</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.85</td>\n",
       "      <td>3</td>\n",
       "      <td>237</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11993</th>\n",
       "      <td>0.64</td>\n",
       "      <td>0.50</td>\n",
       "      <td>4</td>\n",
       "      <td>253</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11994</th>\n",
       "      <td>0.22</td>\n",
       "      <td>0.94</td>\n",
       "      <td>3</td>\n",
       "      <td>193</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11995</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.55</td>\n",
       "      <td>3</td>\n",
       "      <td>259</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11996</th>\n",
       "      <td>0.74</td>\n",
       "      <td>0.95</td>\n",
       "      <td>5</td>\n",
       "      <td>266</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11997</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.54</td>\n",
       "      <td>3</td>\n",
       "      <td>185</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11998</th>\n",
       "      <td>0.33</td>\n",
       "      <td>0.65</td>\n",
       "      <td>3</td>\n",
       "      <td>172</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11999</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.73</td>\n",
       "      <td>4</td>\n",
       "      <td>180</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12000 rows Ã— 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       satisfaction_level  last_evaluation  number_project  \\\n",
       "0                    0.38             0.53               2   \n",
       "1                    0.80             0.86               5   \n",
       "2                    0.11             0.88               7   \n",
       "3                    0.72             0.87               5   \n",
       "4                    0.37             0.52               2   \n",
       "5                    0.41             0.50               2   \n",
       "6                    0.10             0.77               6   \n",
       "7                    0.92             0.85               5   \n",
       "8                    0.89             1.00               5   \n",
       "9                    0.42             0.53               2   \n",
       "10                   0.45             0.54               2   \n",
       "11                   0.11             0.81               6   \n",
       "12                   0.84             0.92               4   \n",
       "13                   0.41             0.55               2   \n",
       "14                   0.36             0.56               2   \n",
       "15                   0.38             0.54               2   \n",
       "16                   0.45             0.47               2   \n",
       "17                   0.78             0.99               4   \n",
       "18                   0.45             0.51               2   \n",
       "19                   0.76             0.89               5   \n",
       "20                   0.11             0.83               6   \n",
       "21                   0.38             0.55               2   \n",
       "22                   0.09             0.95               6   \n",
       "23                   0.46             0.57               2   \n",
       "24                   0.40             0.53               2   \n",
       "25                   0.89             0.92               5   \n",
       "26                   0.82             0.87               4   \n",
       "27                   0.40             0.49               2   \n",
       "28                   0.41             0.46               2   \n",
       "29                   0.38             0.50               2   \n",
       "...                   ...              ...             ...   \n",
       "11970                0.14             0.38               5   \n",
       "11971                0.85             0.89               4   \n",
       "11972                0.55             0.81               3   \n",
       "11973                0.49             0.71               4   \n",
       "11974                0.82             0.58               5   \n",
       "11975                0.59             0.77               3   \n",
       "11976                0.90             0.82               3   \n",
       "11977                0.62             0.72               3   \n",
       "11978                0.61             0.68               3   \n",
       "11979                0.52             0.55               5   \n",
       "11980                0.79             0.87               4   \n",
       "11981                0.49             0.89               4   \n",
       "11982                0.73             0.67               2   \n",
       "11983                0.67             0.49               5   \n",
       "11984                0.52             0.61               4   \n",
       "11985                0.72             0.64               4   \n",
       "11986                0.48             0.50               5   \n",
       "11987                0.19             0.79               4   \n",
       "11988                0.49             0.49               3   \n",
       "11989                0.90             0.76               3   \n",
       "11990                0.49             0.49               4   \n",
       "11991                0.60             0.53               2   \n",
       "11992                0.62             0.85               3   \n",
       "11993                0.64             0.50               4   \n",
       "11994                0.22             0.94               3   \n",
       "11995                0.90             0.55               3   \n",
       "11996                0.74             0.95               5   \n",
       "11997                0.85             0.54               3   \n",
       "11998                0.33             0.65               3   \n",
       "11999                0.50             0.73               4   \n",
       "\n",
       "       average_montly_hours  time_spend_company  Work_accident  \\\n",
       "0                       157                   3              0   \n",
       "1                       262                   6              0   \n",
       "2                       272                   4              0   \n",
       "3                       223                   5              0   \n",
       "4                       159                   3              0   \n",
       "5                       153                   3              0   \n",
       "6                       247                   4              0   \n",
       "7                       259                   5              0   \n",
       "8                       224                   5              0   \n",
       "9                       142                   3              0   \n",
       "10                      135                   3              0   \n",
       "11                      305                   4              0   \n",
       "12                      234                   5              0   \n",
       "13                      148                   3              0   \n",
       "14                      137                   3              0   \n",
       "15                      143                   3              0   \n",
       "16                      160                   3              0   \n",
       "17                      255                   6              0   \n",
       "18                      160                   3              1   \n",
       "19                      262                   5              0   \n",
       "20                      282                   4              0   \n",
       "21                      147                   3              0   \n",
       "22                      304                   4              0   \n",
       "23                      139                   3              0   \n",
       "24                      158                   3              0   \n",
       "25                      242                   5              0   \n",
       "26                      239                   5              0   \n",
       "27                      135                   3              0   \n",
       "28                      128                   3              0   \n",
       "29                      132                   3              0   \n",
       "...                     ...                 ...            ...   \n",
       "11970                   115                   6              1   \n",
       "11971                   150                   3              0   \n",
       "11972                   239                   8              0   \n",
       "11973                   178                   8              0   \n",
       "11974                   263                   8              0   \n",
       "11975                   272                   8              0   \n",
       "11976                   133                   8              0   \n",
       "11977                   149                   3              1   \n",
       "11978                   193                   2              0   \n",
       "11979                   174                   3              1   \n",
       "11980                   223                   5              0   \n",
       "11981                   201                   8              0   \n",
       "11982                   139                   8              0   \n",
       "11983                   241                   8              0   \n",
       "11984                   187                   4              1   \n",
       "11985                   192                   3              0   \n",
       "11986                   142                   4              0   \n",
       "11987                   229                   4              0   \n",
       "11988                   104                   7              0   \n",
       "11989                   255                   7              0   \n",
       "11990                   212                   7              0   \n",
       "11991                   235                   7              0   \n",
       "11992                   237                   3              1   \n",
       "11993                   253                  10              0   \n",
       "11994                   193                  10              0   \n",
       "11995                   259                  10              1   \n",
       "11996                   266                  10              0   \n",
       "11997                   185                  10              0   \n",
       "11998                   172                  10              0   \n",
       "11999                   180                   3              0   \n",
       "\n",
       "       promotion_last_5years  sales_IT  sales_RandD  sales_accounting  \\\n",
       "0                          0         0            0                 0   \n",
       "1                          0         0            0                 0   \n",
       "2                          0         0            0                 0   \n",
       "3                          0         0            0                 0   \n",
       "4                          0         0            0                 0   \n",
       "5                          0         0            0                 0   \n",
       "6                          0         0            0                 0   \n",
       "7                          0         0            0                 0   \n",
       "8                          0         0            0                 0   \n",
       "9                          0         0            0                 0   \n",
       "10                         0         0            0                 0   \n",
       "11                         0         0            0                 0   \n",
       "12                         0         0            0                 0   \n",
       "13                         0         0            0                 0   \n",
       "14                         0         0            0                 0   \n",
       "15                         0         0            0                 0   \n",
       "16                         0         0            0                 0   \n",
       "17                         0         0            0                 0   \n",
       "18                         1         0            0                 0   \n",
       "19                         0         0            0                 0   \n",
       "20                         0         0            0                 0   \n",
       "21                         0         0            0                 0   \n",
       "22                         0         0            0                 0   \n",
       "23                         0         0            0                 0   \n",
       "24                         0         0            0                 0   \n",
       "25                         0         0            0                 0   \n",
       "26                         0         0            0                 0   \n",
       "27                         0         0            0                 0   \n",
       "28                         0         0            0                 1   \n",
       "29                         0         0            0                 1   \n",
       "...                      ...       ...          ...               ...   \n",
       "11970                      0         0            0                 0   \n",
       "11971                      0         0            0                 1   \n",
       "11972                      0         0            0                 1   \n",
       "11973                      0         1            0                 0   \n",
       "11974                      0         1            0                 0   \n",
       "11975                      0         0            0                 0   \n",
       "11976                      0         0            0                 0   \n",
       "11977                      0         0            0                 0   \n",
       "11978                      0         0            0                 0   \n",
       "11979                      0         0            0                 0   \n",
       "11980                      0         0            0                 0   \n",
       "11981                      0         0            0                 0   \n",
       "11982                      0         0            0                 0   \n",
       "11983                      0         0            0                 0   \n",
       "11984                      0         0            0                 0   \n",
       "11985                      0         0            0                 0   \n",
       "11986                      0         1            0                 0   \n",
       "11987                      0         0            0                 0   \n",
       "11988                      0         0            0                 0   \n",
       "11989                      0         0            0                 0   \n",
       "11990                      0         0            0                 0   \n",
       "11991                      0         1            0                 0   \n",
       "11992                      0         1            0                 0   \n",
       "11993                      1         0            0                 0   \n",
       "11994                      1         0            0                 0   \n",
       "11995                      1         0            0                 0   \n",
       "11996                      1         0            0                 0   \n",
       "11997                      1         0            0                 0   \n",
       "11998                      1         0            0                 0   \n",
       "11999                      0         1            0                 0   \n",
       "\n",
       "       sales_hr  sales_management  sales_marketing  sales_product_mng  \\\n",
       "0             0                 0                0                  0   \n",
       "1             0                 0                0                  0   \n",
       "2             0                 0                0                  0   \n",
       "3             0                 0                0                  0   \n",
       "4             0                 0                0                  0   \n",
       "5             0                 0                0                  0   \n",
       "6             0                 0                0                  0   \n",
       "7             0                 0                0                  0   \n",
       "8             0                 0                0                  0   \n",
       "9             0                 0                0                  0   \n",
       "10            0                 0                0                  0   \n",
       "11            0                 0                0                  0   \n",
       "12            0                 0                0                  0   \n",
       "13            0                 0                0                  0   \n",
       "14            0                 0                0                  0   \n",
       "15            0                 0                0                  0   \n",
       "16            0                 0                0                  0   \n",
       "17            0                 0                0                  0   \n",
       "18            0                 0                0                  0   \n",
       "19            0                 0                0                  0   \n",
       "20            0                 0                0                  0   \n",
       "21            0                 0                0                  0   \n",
       "22            0                 0                0                  0   \n",
       "23            0                 0                0                  0   \n",
       "24            0                 0                0                  0   \n",
       "25            0                 0                0                  0   \n",
       "26            0                 0                0                  0   \n",
       "27            0                 0                0                  0   \n",
       "28            0                 0                0                  0   \n",
       "29            0                 0                0                  0   \n",
       "...         ...               ...              ...                ...   \n",
       "11970         0                 0                1                  0   \n",
       "11971         0                 0                0                  0   \n",
       "11972         0                 0                0                  0   \n",
       "11973         0                 0                0                  0   \n",
       "11974         0                 0                0                  0   \n",
       "11975         0                 1                0                  0   \n",
       "11976         0                 0                1                  0   \n",
       "11977         0                 0                1                  0   \n",
       "11978         0                 0                1                  0   \n",
       "11979         0                 0                0                  0   \n",
       "11980         0                 0                0                  0   \n",
       "11981         0                 0                0                  0   \n",
       "11982         0                 0                0                  0   \n",
       "11983         0                 0                0                  0   \n",
       "11984         0                 0                0                  0   \n",
       "11985         0                 0                0                  0   \n",
       "11986         0                 0                0                  0   \n",
       "11987         0                 0                0                  1   \n",
       "11988         0                 0                0                  1   \n",
       "11989         0                 0                0                  1   \n",
       "11990         0                 0                0                  1   \n",
       "11991         0                 0                0                  0   \n",
       "11992         0                 0                0                  0   \n",
       "11993         0                 1                0                  0   \n",
       "11994         0                 1                0                  0   \n",
       "11995         0                 1                0                  0   \n",
       "11996         0                 1                0                  0   \n",
       "11997         0                 1                0                  0   \n",
       "11998         0                 0                1                  0   \n",
       "11999         0                 0                0                  0   \n",
       "\n",
       "       sales_sales  sales_support  sales_technical  salary_high  salary_low  \\\n",
       "0                1              0                0            0           1   \n",
       "1                1              0                0            0           0   \n",
       "2                1              0                0            0           0   \n",
       "3                1              0                0            0           1   \n",
       "4                1              0                0            0           1   \n",
       "5                1              0                0            0           1   \n",
       "6                1              0                0            0           1   \n",
       "7                1              0                0            0           1   \n",
       "8                1              0                0            0           1   \n",
       "9                1              0                0            0           1   \n",
       "10               1              0                0            0           1   \n",
       "11               1              0                0            0           1   \n",
       "12               1              0                0            0           1   \n",
       "13               1              0                0            0           1   \n",
       "14               1              0                0            0           1   \n",
       "15               1              0                0            0           1   \n",
       "16               1              0                0            0           1   \n",
       "17               1              0                0            0           1   \n",
       "18               1              0                0            0           1   \n",
       "19               1              0                0            0           1   \n",
       "20               1              0                0            0           1   \n",
       "21               1              0                0            0           1   \n",
       "22               1              0                0            0           1   \n",
       "23               1              0                0            0           1   \n",
       "24               1              0                0            0           1   \n",
       "25               1              0                0            0           1   \n",
       "26               1              0                0            0           1   \n",
       "27               1              0                0            0           1   \n",
       "28               0              0                0            0           1   \n",
       "29               0              0                0            0           1   \n",
       "...            ...            ...              ...          ...         ...   \n",
       "11970            0              0                0            1           0   \n",
       "11971            0              0                0            1           0   \n",
       "11972            0              0                0            1           0   \n",
       "11973            0              0                0            0           0   \n",
       "11974            0              0                0            0           0   \n",
       "11975            0              0                0            1           0   \n",
       "11976            0              0                0            0           0   \n",
       "11977            0              0                0            0           0   \n",
       "11978            0              0                0            0           0   \n",
       "11979            1              0                0            0           0   \n",
       "11980            1              0                0            0           0   \n",
       "11981            1              0                0            0           0   \n",
       "11982            1              0                0            0           0   \n",
       "11983            1              0                0            0           0   \n",
       "11984            1              0                0            0           0   \n",
       "11985            1              0                0            0           0   \n",
       "11986            0              0                0            0           0   \n",
       "11987            0              0                0            0           0   \n",
       "11988            0              0                0            1           0   \n",
       "11989            0              0                0            0           1   \n",
       "11990            0              0                0            0           0   \n",
       "11991            0              0                0            0           0   \n",
       "11992            0              0                0            0           0   \n",
       "11993            0              0                0            1           0   \n",
       "11994            0              0                0            1           0   \n",
       "11995            0              0                0            1           0   \n",
       "11996            0              0                0            1           0   \n",
       "11997            0              0                0            1           0   \n",
       "11998            0              0                0            1           0   \n",
       "11999            0              0                0            0           1   \n",
       "\n",
       "       salary_medium  \n",
       "0                  0  \n",
       "1                  1  \n",
       "2                  1  \n",
       "3                  0  \n",
       "4                  0  \n",
       "5                  0  \n",
       "6                  0  \n",
       "7                  0  \n",
       "8                  0  \n",
       "9                  0  \n",
       "10                 0  \n",
       "11                 0  \n",
       "12                 0  \n",
       "13                 0  \n",
       "14                 0  \n",
       "15                 0  \n",
       "16                 0  \n",
       "17                 0  \n",
       "18                 0  \n",
       "19                 0  \n",
       "20                 0  \n",
       "21                 0  \n",
       "22                 0  \n",
       "23                 0  \n",
       "24                 0  \n",
       "25                 0  \n",
       "26                 0  \n",
       "27                 0  \n",
       "28                 0  \n",
       "29                 0  \n",
       "...              ...  \n",
       "11970              0  \n",
       "11971              0  \n",
       "11972              0  \n",
       "11973              1  \n",
       "11974              1  \n",
       "11975              0  \n",
       "11976              1  \n",
       "11977              1  \n",
       "11978              1  \n",
       "11979              1  \n",
       "11980              1  \n",
       "11981              1  \n",
       "11982              1  \n",
       "11983              1  \n",
       "11984              1  \n",
       "11985              1  \n",
       "11986              1  \n",
       "11987              1  \n",
       "11988              0  \n",
       "11989              0  \n",
       "11990              1  \n",
       "11991              1  \n",
       "11992              1  \n",
       "11993              0  \n",
       "11994              0  \n",
       "11995              0  \n",
       "11996              0  \n",
       "11997              0  \n",
       "11998              0  \n",
       "11999              0  \n",
       "\n",
       "[12000 rows x 20 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict['x_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** 81 configurations x 1.0 iterations each\n",
      "\n",
      "1 | Thu Sep 28 00:13:23 2017 | lowest loss so far: inf (run -1)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.10948073658267082,\n",
      " 'max_depth': 2,\n",
      " 'max_features': None,\n",
      " 'min_samples_leaf': 5,\n",
      " 'min_samples_split': 19,\n",
      " 'subsample': 0.9773564962320919}\n",
      "\n",
      "# training | log loss: 50.17%, AUC: 91.96%, accuracy: 91.17%\n",
      "# testing  | log loss: 50.95%, AUC: 91.37%, accuracy: 88.22%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "2 | Thu Sep 28 00:13:23 2017 | lowest loss so far: 0.5095 (run 1)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.058899244976344516,\n",
      " 'max_depth': 10,\n",
      " 'max_features': 'log2',\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 11,\n",
      " 'subsample': 0.8654569276546461}\n",
      "\n",
      "# training | log loss: 49.09%, AUC: 99.73%, accuracy: 97.83%\n",
      "# testing  | log loss: 52.74%, AUC: 98.57%, accuracy: 95.90%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "3 | Thu Sep 28 00:13:23 2017 | lowest loss so far: 0.5095 (run 1)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.10448357968405217,\n",
      " 'max_depth': 4,\n",
      " 'max_features': 'sqrt',\n",
      " 'min_samples_leaf': 4,\n",
      " 'min_samples_split': 4,\n",
      " 'subsample': 0.8734034479138724}\n",
      "\n",
      "# training | log loss: 47.34%, AUC: 97.45%, accuracy: 92.53%\n",
      "# testing  | log loss: 49.19%, AUC: 96.95%, accuracy: 90.22%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "4 | Thu Sep 28 00:13:23 2017 | lowest loss so far: 0.4919 (run 3)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.15609784942438312,\n",
      " 'max_depth': 3,\n",
      " 'max_features': None,\n",
      " 'min_samples_leaf': 8,\n",
      " 'min_samples_split': 19,\n",
      " 'subsample': 0.9458587456887256}\n",
      "\n",
      "# training | log loss: 37.56%, AUC: 95.64%, accuracy: 92.17%\n",
      "# testing  | log loss: 40.79%, AUC: 94.38%, accuracy: 89.10%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "5 | Thu Sep 28 00:13:23 2017 | lowest loss so far: 0.4079 (run 4)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.17069686082741595,\n",
      " 'max_depth': 9,\n",
      " 'max_features': None,\n",
      " 'min_samples_leaf': 3,\n",
      " 'min_samples_split': 6,\n",
      " 'subsample': 0.8139185851430316}\n",
      "\n",
      "# training | log loss: 25.91%, AUC: 99.87%, accuracy: 98.53%\n",
      "# testing  | log loss: 30.90%, AUC: 98.30%, accuracy: 95.47%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "6 | Thu Sep 28 00:13:23 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.14323745355368028,\n",
      " 'max_depth': 7,\n",
      " 'max_features': 'sqrt',\n",
      " 'min_samples_leaf': 2,\n",
      " 'min_samples_split': 19,\n",
      " 'subsample': 0.9789507029177924}\n",
      "\n",
      "# training | log loss: 36.89%, AUC: 99.04%, accuracy: 95.57%\n",
      "# testing  | log loss: 40.09%, AUC: 98.02%, accuracy: 94.57%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "7 | Thu Sep 28 00:13:23 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.10289948874412463,\n",
      " 'max_depth': 9,\n",
      " 'max_features': None,\n",
      " 'min_samples_leaf': 10,\n",
      " 'min_samples_split': 15,\n",
      " 'subsample': 0.9346719632545584}\n",
      "\n",
      "# training | log loss: 38.48%, AUC: 99.67%, accuracy: 96.73%\n",
      "# testing  | log loss: 42.15%, AUC: 98.22%, accuracy: 94.75%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "8 | Thu Sep 28 00:13:23 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.03658559355455769,\n",
      " 'max_depth': 7,\n",
      " 'max_features': None,\n",
      " 'min_samples_leaf': 3,\n",
      " 'min_samples_split': 11,\n",
      " 'subsample': 0.9397067874103712}\n",
      "\n",
      "# training | log loss: 55.36%, AUC: 99.34%, accuracy: 96.20%\n",
      "# testing  | log loss: 58.99%, AUC: 98.16%, accuracy: 90.14%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "9 | Thu Sep 28 00:13:23 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.1036143909806879,\n",
      " 'max_depth': 5,\n",
      " 'max_features': None,\n",
      " 'min_samples_leaf': 3,\n",
      " 'min_samples_split': 2,\n",
      " 'subsample': 0.9022649851727861}\n",
      "\n",
      "# training | log loss: 40.55%, AUC: 98.70%, accuracy: 95.07%\n",
      "# testing  | log loss: 44.06%, AUC: 97.98%, accuracy: 92.70%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "10 | Thu Sep 28 00:13:23 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.18861368617982344,\n",
      " 'max_depth': 5,\n",
      " 'max_features': 'sqrt',\n",
      " 'min_samples_leaf': 9,\n",
      " 'min_samples_split': 11,\n",
      " 'subsample': 0.8733090008355578}\n",
      "\n",
      "# training | log loss: 37.12%, AUC: 97.51%, accuracy: 93.10%\n",
      "# testing  | log loss: 39.92%, AUC: 96.65%, accuracy: 90.90%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "11 | Thu Sep 28 00:13:23 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.07819648181916913,\n",
      " 'max_depth': 7,\n",
      " 'max_features': 'log2',\n",
      " 'min_samples_leaf': 7,\n",
      " 'min_samples_split': 18,\n",
      " 'subsample': 0.9677501623270475}\n",
      "\n",
      "# training | log loss: 46.50%, AUC: 99.09%, accuracy: 94.97%\n",
      "# testing  | log loss: 49.29%, AUC: 98.19%, accuracy: 95.43%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "12 | Thu Sep 28 00:13:24 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.10434608771666079,\n",
      " 'max_depth': 10,\n",
      " 'max_features': None,\n",
      " 'min_samples_leaf': 6,\n",
      " 'min_samples_split': 9,\n",
      " 'subsample': 0.8194257546806554}\n",
      "\n",
      "# training | log loss: 37.57%, AUC: 99.79%, accuracy: 97.43%\n",
      "# testing  | log loss: 41.40%, AUC: 98.22%, accuracy: 94.90%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "13 | Thu Sep 28 00:13:24 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.19231464207271945,\n",
      " 'max_depth': 4,\n",
      " 'max_features': 'sqrt',\n",
      " 'min_samples_leaf': 4,\n",
      " 'min_samples_split': 15,\n",
      " 'subsample': 0.9570981494669738}\n",
      "\n",
      "# training | log loss: 35.27%, AUC: 97.18%, accuracy: 93.03%\n",
      "# testing  | log loss: 35.60%, AUC: 97.16%, accuracy: 94.62%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "14 | Thu Sep 28 00:13:24 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.04494947937979423,\n",
      " 'max_depth': 2,\n",
      " 'max_features': 'sqrt',\n",
      " 'min_samples_leaf': 9,\n",
      " 'min_samples_split': 17,\n",
      " 'subsample': 0.8552432432042557}\n",
      "\n",
      "# training | log loss: 62.82%, AUC: 94.53%, accuracy: 87.97%\n",
      "# testing  | log loss: 65.95%, AUC: 93.74%, accuracy: 80.15%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "15 | Thu Sep 28 00:13:24 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.06963540543105548,\n",
      " 'max_depth': 8,\n",
      " 'max_features': None,\n",
      " 'min_samples_leaf': 7,\n",
      " 'min_samples_split': 11,\n",
      " 'subsample': 0.8188478122070628}\n",
      "\n",
      "# training | log loss: 45.69%, AUC: 99.67%, accuracy: 97.33%\n",
      "# testing  | log loss: 49.22%, AUC: 98.32%, accuracy: 94.49%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "16 | Thu Sep 28 00:13:24 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.058952087381017336,\n",
      " 'max_depth': 6,\n",
      " 'max_features': 'sqrt',\n",
      " 'min_samples_leaf': 7,\n",
      " 'min_samples_split': 5,\n",
      " 'subsample': 0.9966163338820628}\n",
      "\n",
      "# training | log loss: 52.63%, AUC: 98.29%, accuracy: 94.10%\n",
      "# testing  | log loss: 56.48%, AUC: 97.38%, accuracy: 91.07%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "17 | Thu Sep 28 00:13:24 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.030837502624209374,\n",
      " 'max_depth': 3,\n",
      " 'max_features': None,\n",
      " 'min_samples_leaf': 2,\n",
      " 'min_samples_split': 7,\n",
      " 'subsample': 0.9905455755391733}\n",
      "\n",
      "# training | log loss: 59.65%, AUC: 94.53%, accuracy: 91.63%\n",
      "# testing  | log loss: 63.10%, AUC: 93.11%, accuracy: 86.38%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "18 | Thu Sep 28 00:13:24 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.04495293667927852,\n",
      " 'max_depth': 2,\n",
      " 'max_features': 'sqrt',\n",
      " 'min_samples_leaf': 5,\n",
      " 'min_samples_split': 14,\n",
      " 'subsample': 0.8483862634873441}\n",
      "\n",
      "# training | log loss: 65.21%, AUC: 90.58%, accuracy: 69.80%\n",
      "# testing  | log loss: 69.11%, AUC: 87.29%, accuracy: 41.97%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "19 | Thu Sep 28 00:13:24 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.10930996743854532,\n",
      " 'max_depth': 7,\n",
      " 'max_features': 'log2',\n",
      " 'min_samples_leaf': 5,\n",
      " 'min_samples_split': 10,\n",
      " 'subsample': 0.9661972027540967}\n",
      "\n",
      "# training | log loss: 43.38%, AUC: 98.99%, accuracy: 95.63%\n",
      "# testing  | log loss: 47.81%, AUC: 97.76%, accuracy: 93.55%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "20 | Thu Sep 28 00:13:24 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.11018803449152438,\n",
      " 'max_depth': 3,\n",
      " 'max_features': 'sqrt',\n",
      " 'min_samples_leaf': 6,\n",
      " 'min_samples_split': 3,\n",
      " 'subsample': 0.9427879306033327}\n",
      "\n",
      "# training | log loss: 50.19%, AUC: 96.11%, accuracy: 89.30%\n",
      "# testing  | log loss: 51.87%, AUC: 95.71%, accuracy: 87.36%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "21 | Thu Sep 28 00:13:24 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.15485009197981445,\n",
      " 'max_depth': 10,\n",
      " 'max_features': 'log2',\n",
      " 'min_samples_leaf': 6,\n",
      " 'min_samples_split': 11,\n",
      " 'subsample': 0.8229184707116074}\n",
      "\n",
      "# training | log loss: 32.13%, AUC: 99.54%, accuracy: 96.70%\n",
      "# testing  | log loss: 36.10%, AUC: 98.24%, accuracy: 95.41%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "22 | Thu Sep 28 00:13:24 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.14896584649131023,\n",
      " 'max_depth': 3,\n",
      " 'max_features': 'log2',\n",
      " 'min_samples_leaf': 6,\n",
      " 'min_samples_split': 12,\n",
      " 'subsample': 0.9658775789497799}\n",
      "\n",
      "# training | log loss: 50.32%, AUC: 96.58%, accuracy: 92.43%\n",
      "# testing  | log loss: 54.40%, AUC: 95.54%, accuracy: 87.83%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "23 | Thu Sep 28 00:13:24 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.08102811018330158,\n",
      " 'max_depth': 7,\n",
      " 'max_features': 'sqrt',\n",
      " 'min_samples_leaf': 3,\n",
      " 'min_samples_split': 10,\n",
      " 'subsample': 0.8032146428206114}\n",
      "\n",
      "# training | log loss: 46.73%, AUC: 98.78%, accuracy: 94.83%\n",
      "# testing  | log loss: 49.75%, AUC: 97.79%, accuracy: 94.13%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "24 | Thu Sep 28 00:13:24 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.1460354004351075,\n",
      " 'max_depth': 6,\n",
      " 'max_features': 'log2',\n",
      " 'min_samples_leaf': 4,\n",
      " 'min_samples_split': 4,\n",
      " 'subsample': 0.9848457686458227}\n",
      "\n",
      "# training | log loss: 37.62%, AUC: 98.86%, accuracy: 95.27%\n",
      "# testing  | log loss: 40.98%, AUC: 97.70%, accuracy: 95.08%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "25 | Thu Sep 28 00:13:24 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.05379983451678785,\n",
      " 'max_depth': 2,\n",
      " 'max_features': 'sqrt',\n",
      " 'min_samples_leaf': 4,\n",
      " 'min_samples_split': 6,\n",
      " 'subsample': 0.8473913523401649}\n",
      "\n",
      "# training | log loss: 62.70%, AUC: 90.45%, accuracy: 85.13%\n",
      "# testing  | log loss: 66.26%, AUC: 89.17%, accuracy: 83.67%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "26 | Thu Sep 28 00:13:24 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.15583855270255634,\n",
      " 'max_depth': 3,\n",
      " 'max_features': None,\n",
      " 'min_samples_leaf': 3,\n",
      " 'min_samples_split': 10,\n",
      " 'subsample': 0.9193392775510302}\n",
      "\n",
      "# training | log loss: 37.47%, AUC: 96.20%, accuracy: 92.30%\n",
      "# testing  | log loss: 40.49%, AUC: 95.39%, accuracy: 89.14%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "27 | Thu Sep 28 00:13:24 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.18435730545244297,\n",
      " 'max_depth': 5,\n",
      " 'max_features': 'log2',\n",
      " 'min_samples_leaf': 7,\n",
      " 'min_samples_split': 19,\n",
      " 'subsample': 0.9724838814069894}\n",
      "\n",
      "# training | log loss: 33.61%, AUC: 98.32%, accuracy: 94.00%\n",
      "# testing  | log loss: 36.75%, AUC: 97.70%, accuracy: 92.37%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "28 | Thu Sep 28 00:13:24 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.04088893690608235,\n",
      " 'max_depth': 3,\n",
      " 'max_features': 'sqrt',\n",
      " 'min_samples_leaf': 2,\n",
      " 'min_samples_split': 18,\n",
      " 'subsample': 0.9889510222577614}\n",
      "\n",
      "# training | log loss: 61.31%, AUC: 95.44%, accuracy: 87.73%\n",
      "# testing  | log loss: 64.63%, AUC: 94.55%, accuracy: 80.12%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "29 | Thu Sep 28 00:13:24 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.09575315309459137,\n",
      " 'max_depth': 3,\n",
      " 'max_features': 'sqrt',\n",
      " 'min_samples_leaf': 2,\n",
      " 'min_samples_split': 17,\n",
      " 'subsample': 0.8689239383594627}\n",
      "\n",
      "# training | log loss: 53.73%, AUC: 96.24%, accuracy: 90.80%\n",
      "# testing  | log loss: 56.42%, AUC: 95.17%, accuracy: 89.02%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "30 | Thu Sep 28 00:13:24 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.06114858379194291,\n",
      " 'max_depth': 10,\n",
      " 'max_features': 'log2',\n",
      " 'min_samples_leaf': 7,\n",
      " 'min_samples_split': 17,\n",
      " 'subsample': 0.8555222181396234}\n",
      "\n",
      "# training | log loss: 49.59%, AUC: 99.42%, accuracy: 96.10%\n",
      "# testing  | log loss: 53.13%, AUC: 98.06%, accuracy: 94.97%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "31 | Thu Sep 28 00:13:24 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.032630983224506215,\n",
      " 'max_depth': 9,\n",
      " 'max_features': 'log2',\n",
      " 'min_samples_leaf': 8,\n",
      " 'min_samples_split': 5,\n",
      " 'subsample': 0.8327860211439186}\n",
      "\n",
      "# training | log loss: 58.42%, AUC: 99.19%, accuracy: 94.93%\n",
      "# testing  | log loss: 61.70%, AUC: 98.09%, accuracy: 90.89%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "32 | Thu Sep 28 00:13:25 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.03355905400659223,\n",
      " 'max_depth': 8,\n",
      " 'max_features': None,\n",
      " 'min_samples_leaf': 8,\n",
      " 'min_samples_split': 6,\n",
      " 'subsample': 0.8256680296984114}\n",
      "\n",
      "# training | log loss: 56.40%, AUC: 99.56%, accuracy: 96.53%\n",
      "# testing  | log loss: 59.71%, AUC: 98.42%, accuracy: 92.88%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "33 | Thu Sep 28 00:13:25 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.13987247521336282,\n",
      " 'max_depth': 8,\n",
      " 'max_features': 'log2',\n",
      " 'min_samples_leaf': 7,\n",
      " 'min_samples_split': 4,\n",
      " 'subsample': 0.9389281611550853}\n",
      "\n",
      "# training | log loss: 34.66%, AUC: 99.28%, accuracy: 95.60%\n",
      "# testing  | log loss: 38.12%, AUC: 98.17%, accuracy: 96.05%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "34 | Thu Sep 28 00:13:25 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.1806841170282773,\n",
      " 'max_depth': 10,\n",
      " 'max_features': 'sqrt',\n",
      " 'min_samples_leaf': 6,\n",
      " 'min_samples_split': 9,\n",
      " 'subsample': 0.8191090250789204}\n",
      "\n",
      "# training | log loss: 28.53%, AUC: 99.65%, accuracy: 96.93%\n",
      "# testing  | log loss: 32.29%, AUC: 98.29%, accuracy: 95.80%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "35 | Thu Sep 28 00:13:25 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.06251156148448722,\n",
      " 'max_depth': 4,\n",
      " 'max_features': 'sqrt',\n",
      " 'min_samples_leaf': 3,\n",
      " 'min_samples_split': 6,\n",
      " 'subsample': 0.9476898184305961}\n",
      "\n",
      "# training | log loss: 54.27%, AUC: 96.83%, accuracy: 91.53%\n",
      "# testing  | log loss: 57.33%, AUC: 96.18%, accuracy: 87.90%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "36 | Thu Sep 28 00:13:25 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.11337387191851031,\n",
      " 'max_depth': 6,\n",
      " 'max_features': 'log2',\n",
      " 'min_samples_leaf': 4,\n",
      " 'min_samples_split': 20,\n",
      " 'subsample': 0.8599813587228342}\n",
      "\n",
      "# training | log loss: 41.59%, AUC: 98.63%, accuracy: 94.53%\n",
      "# testing  | log loss: 44.09%, AUC: 97.90%, accuracy: 93.55%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "37 | Thu Sep 28 00:13:25 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.10112729180983848,\n",
      " 'max_depth': 8,\n",
      " 'max_features': 'sqrt',\n",
      " 'min_samples_leaf': 7,\n",
      " 'min_samples_split': 14,\n",
      " 'subsample': 0.8344054228561553}\n",
      "\n",
      "# training | log loss: 42.10%, AUC: 99.09%, accuracy: 96.00%\n",
      "# testing  | log loss: 45.35%, AUC: 98.12%, accuracy: 94.92%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "38 | Thu Sep 28 00:13:25 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.11475130940971301,\n",
      " 'max_depth': 4,\n",
      " 'max_features': 'sqrt',\n",
      " 'min_samples_leaf': 7,\n",
      " 'min_samples_split': 8,\n",
      " 'subsample': 0.9383471680271216}\n",
      "\n",
      "# training | log loss: 44.84%, AUC: 98.23%, accuracy: 92.57%\n",
      "# testing  | log loss: 47.57%, AUC: 97.52%, accuracy: 87.08%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "39 | Thu Sep 28 00:13:25 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.03832868741403112,\n",
      " 'max_depth': 5,\n",
      " 'max_features': 'sqrt',\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 20,\n",
      " 'subsample': 0.8888156620111787}\n",
      "\n",
      "# training | log loss: 58.82%, AUC: 97.59%, accuracy: 90.30%\n",
      "# testing  | log loss: 61.53%, AUC: 97.09%, accuracy: 85.07%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "40 | Thu Sep 28 00:13:25 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.05196647788590255,\n",
      " 'max_depth': 6,\n",
      " 'max_features': 'sqrt',\n",
      " 'min_samples_leaf': 9,\n",
      " 'min_samples_split': 11,\n",
      " 'subsample': 0.8599211029331365}\n",
      "\n",
      "# training | log loss: 54.60%, AUC: 98.47%, accuracy: 94.00%\n",
      "# testing  | log loss: 57.87%, AUC: 97.61%, accuracy: 90.08%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "41 | Thu Sep 28 00:13:25 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.19398846486298116,\n",
      " 'max_depth': 5,\n",
      " 'max_features': 'sqrt',\n",
      " 'min_samples_leaf': 2,\n",
      " 'min_samples_split': 16,\n",
      " 'subsample': 0.8796724559223199}\n",
      "\n",
      "# training | log loss: 37.32%, AUC: 97.46%, accuracy: 92.47%\n",
      "# testing  | log loss: 41.45%, AUC: 96.04%, accuracy: 89.13%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "42 | Thu Sep 28 00:13:25 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.15042194352721044,\n",
      " 'max_depth': 10,\n",
      " 'max_features': 'log2',\n",
      " 'min_samples_leaf': 5,\n",
      " 'min_samples_split': 7,\n",
      " 'subsample': 0.8774323852562586}\n",
      "\n",
      "# training | log loss: 32.09%, AUC: 99.63%, accuracy: 97.03%\n",
      "# testing  | log loss: 35.54%, AUC: 98.47%, accuracy: 96.11%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "43 | Thu Sep 28 00:13:25 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.09289395903036422,\n",
      " 'max_depth': 6,\n",
      " 'max_features': None,\n",
      " 'min_samples_leaf': 4,\n",
      " 'min_samples_split': 15,\n",
      " 'subsample': 0.80628739321905}\n",
      "\n",
      "# training | log loss: 42.03%, AUC: 98.86%, accuracy: 95.33%\n",
      "# testing  | log loss: 46.02%, AUC: 97.94%, accuracy: 91.92%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "44 | Thu Sep 28 00:13:25 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.12051688243322352,\n",
      " 'max_depth': 4,\n",
      " 'max_features': 'log2',\n",
      " 'min_samples_leaf': 3,\n",
      " 'min_samples_split': 12,\n",
      " 'subsample': 0.9317533624957337}\n",
      "\n",
      "# training | log loss: 51.02%, AUC: 96.55%, accuracy: 89.87%\n",
      "# testing  | log loss: 56.07%, AUC: 94.24%, accuracy: 82.43%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "45 | Thu Sep 28 00:13:25 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.16680923901424993,\n",
      " 'max_depth': 8,\n",
      " 'max_features': 'log2',\n",
      " 'min_samples_leaf': 8,\n",
      " 'min_samples_split': 14,\n",
      " 'subsample': 0.9039515692026208}\n",
      "\n",
      "# training | log loss: 32.14%, AUC: 99.12%, accuracy: 95.83%\n",
      "# testing  | log loss: 36.00%, AUC: 98.08%, accuracy: 94.04%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "46 | Thu Sep 28 00:13:25 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.11416623794748747,\n",
      " 'max_depth': 9,\n",
      " 'max_features': 'log2',\n",
      " 'min_samples_leaf': 10,\n",
      " 'min_samples_split': 19,\n",
      " 'subsample': 0.8365481052945872}\n",
      "\n",
      "# training | log loss: 40.11%, AUC: 99.07%, accuracy: 95.47%\n",
      "# testing  | log loss: 43.04%, AUC: 98.08%, accuracy: 95.60%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "47 | Thu Sep 28 00:13:25 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.07543728186591725,\n",
      " 'max_depth': 3,\n",
      " 'max_features': 'sqrt',\n",
      " 'min_samples_leaf': 8,\n",
      " 'min_samples_split': 18,\n",
      " 'subsample': 0.828716995241443}\n",
      "\n",
      "# training | log loss: 53.98%, AUC: 96.46%, accuracy: 92.07%\n",
      "# testing  | log loss: 55.16%, AUC: 96.57%, accuracy: 93.72%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "48 | Thu Sep 28 00:13:25 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.18062518717819076,\n",
      " 'max_depth': 3,\n",
      " 'max_features': 'sqrt',\n",
      " 'min_samples_leaf': 5,\n",
      " 'min_samples_split': 8,\n",
      " 'subsample': 0.9232808884538488}\n",
      "\n",
      "# training | log loss: 46.35%, AUC: 94.98%, accuracy: 90.57%\n",
      "# testing  | log loss: 51.80%, AUC: 93.07%, accuracy: 85.48%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "49 | Thu Sep 28 00:13:25 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.012493809480647353,\n",
      " 'max_depth': 6,\n",
      " 'max_features': 'log2',\n",
      " 'min_samples_leaf': 9,\n",
      " 'min_samples_split': 3,\n",
      " 'subsample': 0.9139591348228023}\n",
      "\n",
      "# training | log loss: 65.12%, AUC: 98.69%, accuracy: 77.30%\n",
      "# testing  | log loss: 68.47%, AUC: 97.83%, accuracy: 56.21%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "50 | Thu Sep 28 00:13:25 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.1213723925009975,\n",
      " 'max_depth': 4,\n",
      " 'max_features': 'log2',\n",
      " 'min_samples_leaf': 6,\n",
      " 'min_samples_split': 19,\n",
      " 'subsample': 0.9172177222032587}\n",
      "\n",
      "# training | log loss: 44.37%, AUC: 98.17%, accuracy: 93.80%\n",
      "# testing  | log loss: 47.96%, AUC: 97.30%, accuracy: 92.25%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "51 | Thu Sep 28 00:13:26 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.11878790593045495,\n",
      " 'max_depth': 6,\n",
      " 'max_features': 'sqrt',\n",
      " 'min_samples_leaf': 6,\n",
      " 'min_samples_split': 11,\n",
      " 'subsample': 0.8045936798307327}\n",
      "\n",
      "# training | log loss: 40.31%, AUC: 98.84%, accuracy: 95.10%\n",
      "# testing  | log loss: 42.92%, AUC: 98.04%, accuracy: 94.63%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "52 | Thu Sep 28 00:13:26 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.1481604664666325,\n",
      " 'max_depth': 7,\n",
      " 'max_features': 'sqrt',\n",
      " 'min_samples_leaf': 2,\n",
      " 'min_samples_split': 15,\n",
      " 'subsample': 0.8638638008105615}\n",
      "\n",
      "# training | log loss: 35.35%, AUC: 99.17%, accuracy: 96.30%\n",
      "# testing  | log loss: 39.53%, AUC: 97.91%, accuracy: 94.93%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "53 | Thu Sep 28 00:13:26 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.1771437578615261,\n",
      " 'max_depth': 6,\n",
      " 'max_features': None,\n",
      " 'min_samples_leaf': 8,\n",
      " 'min_samples_split': 6,\n",
      " 'subsample': 0.8231313694203439}\n",
      "\n",
      "# training | log loss: 28.78%, AUC: 99.04%, accuracy: 95.47%\n",
      "# testing  | log loss: 32.59%, AUC: 98.11%, accuracy: 92.10%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "54 | Thu Sep 28 00:13:26 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.1185457477381868,\n",
      " 'max_depth': 10,\n",
      " 'max_features': 'sqrt',\n",
      " 'min_samples_leaf': 10,\n",
      " 'min_samples_split': 3,\n",
      " 'subsample': 0.8755575296420445}\n",
      "\n",
      "# training | log loss: 37.92%, AUC: 99.39%, accuracy: 95.93%\n",
      "# testing  | log loss: 41.58%, AUC: 98.22%, accuracy: 94.61%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "55 | Thu Sep 28 00:13:26 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.17385220664488754,\n",
      " 'max_depth': 7,\n",
      " 'max_features': 'sqrt',\n",
      " 'min_samples_leaf': 7,\n",
      " 'min_samples_split': 19,\n",
      " 'subsample': 0.8482517645970387}\n",
      "\n",
      "# training | log loss: 32.34%, AUC: 98.92%, accuracy: 95.40%\n",
      "# testing  | log loss: 35.38%, AUC: 98.15%, accuracy: 95.69%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "56 | Thu Sep 28 00:13:26 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.10454331708679752,\n",
      " 'max_depth': 5,\n",
      " 'max_features': 'sqrt',\n",
      " 'min_samples_leaf': 5,\n",
      " 'min_samples_split': 4,\n",
      " 'subsample': 0.842972431603815}\n",
      "\n",
      "# training | log loss: 43.89%, AUC: 98.28%, accuracy: 94.13%\n",
      "# testing  | log loss: 47.53%, AUC: 97.28%, accuracy: 90.89%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "57 | Thu Sep 28 00:13:26 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.11497583866607342,\n",
      " 'max_depth': 6,\n",
      " 'max_features': 'sqrt',\n",
      " 'min_samples_leaf': 3,\n",
      " 'min_samples_split': 9,\n",
      " 'subsample': 0.8261043653430352}\n",
      "\n",
      "# training | log loss: 40.53%, AUC: 98.71%, accuracy: 95.47%\n",
      "# testing  | log loss: 43.46%, AUC: 97.80%, accuracy: 94.05%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "58 | Thu Sep 28 00:13:26 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.011938454802018586,\n",
      " 'max_depth': 6,\n",
      " 'max_features': None,\n",
      " 'min_samples_leaf': 4,\n",
      " 'min_samples_split': 8,\n",
      " 'subsample': 0.8804104038307483}\n",
      "\n",
      "# training | log loss: 64.36%, AUC: 98.94%, accuracy: 92.07%\n",
      "# testing  | log loss: 67.79%, AUC: 98.01%, accuracy: 81.40%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "59 | Thu Sep 28 00:13:26 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.08876426639782299,\n",
      " 'max_depth': 3,\n",
      " 'max_features': None,\n",
      " 'min_samples_leaf': 3,\n",
      " 'min_samples_split': 16,\n",
      " 'subsample': 0.8812360804183025}\n",
      "\n",
      "# training | log loss: 46.80%, AUC: 95.17%, accuracy: 92.30%\n",
      "# testing  | log loss: 50.50%, AUC: 93.91%, accuracy: 88.68%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "60 | Thu Sep 28 00:13:26 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.10215479632853298,\n",
      " 'max_depth': 8,\n",
      " 'max_features': None,\n",
      " 'min_samples_leaf': 7,\n",
      " 'min_samples_split': 17,\n",
      " 'subsample': 0.975396683453934}\n",
      "\n",
      "# training | log loss: 38.54%, AUC: 99.58%, accuracy: 96.57%\n",
      "# testing  | log loss: 42.06%, AUC: 98.37%, accuracy: 93.68%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "61 | Thu Sep 28 00:13:26 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.1616233305323814,\n",
      " 'max_depth': 2,\n",
      " 'max_features': None,\n",
      " 'min_samples_leaf': 2,\n",
      " 'min_samples_split': 7,\n",
      " 'subsample': 0.9179609519456022}\n",
      "\n",
      "# training | log loss: 45.05%, AUC: 92.68%, accuracy: 91.07%\n",
      "# testing  | log loss: 45.21%, AUC: 92.06%, accuracy: 88.17%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "62 | Thu Sep 28 00:13:26 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.1669558400372308,\n",
      " 'max_depth': 2,\n",
      " 'max_features': 'sqrt',\n",
      " 'min_samples_leaf': 9,\n",
      " 'min_samples_split': 18,\n",
      " 'subsample': 0.900307211147659}\n",
      "\n",
      "# training | log loss: 56.42%, AUC: 91.29%, accuracy: 77.10%\n",
      "# testing  | log loss: 63.15%, AUC: 87.76%, accuracy: 53.67%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "63 | Thu Sep 28 00:13:26 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.021753205540516955,\n",
      " 'max_depth': 3,\n",
      " 'max_features': 'log2',\n",
      " 'min_samples_leaf': 4,\n",
      " 'min_samples_split': 8,\n",
      " 'subsample': 0.8272459897816197}\n",
      "\n",
      "# training | log loss: 65.48%, AUC: 92.98%, accuracy: 71.13%\n",
      "# testing  | log loss: 69.20%, AUC: 91.21%, accuracy: 42.24%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "64 | Thu Sep 28 00:13:26 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.1066733053310453,\n",
      " 'max_depth': 6,\n",
      " 'max_features': None,\n",
      " 'min_samples_leaf': 3,\n",
      " 'min_samples_split': 10,\n",
      " 'subsample': 0.9959838070161853}\n",
      "\n",
      "# training | log loss: 38.34%, AUC: 99.07%, accuracy: 95.50%\n",
      "# testing  | log loss: 42.06%, AUC: 97.82%, accuracy: 91.20%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "65 | Thu Sep 28 00:13:26 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.1769846620987672,\n",
      " 'max_depth': 9,\n",
      " 'max_features': 'sqrt',\n",
      " 'min_samples_leaf': 4,\n",
      " 'min_samples_split': 9,\n",
      " 'subsample': 0.9725755848387072}\n",
      "\n",
      "# training | log loss: 28.42%, AUC: 99.68%, accuracy: 96.83%\n",
      "# testing  | log loss: 32.92%, AUC: 98.32%, accuracy: 95.03%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "66 | Thu Sep 28 00:13:26 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.1252669037490443,\n",
      " 'max_depth': 10,\n",
      " 'max_features': 'sqrt',\n",
      " 'min_samples_leaf': 8,\n",
      " 'min_samples_split': 5,\n",
      " 'subsample': 0.9046079334166649}\n",
      "\n",
      "# training | log loss: 35.60%, AUC: 99.58%, accuracy: 96.67%\n",
      "# testing  | log loss: 38.70%, AUC: 98.44%, accuracy: 96.31%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "67 | Thu Sep 28 00:13:26 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.02327624463068417,\n",
      " 'max_depth': 7,\n",
      " 'max_features': 'log2',\n",
      " 'min_samples_leaf': 3,\n",
      " 'min_samples_split': 11,\n",
      " 'subsample': 0.9309274493172528}\n",
      "\n",
      "# training | log loss: 60.89%, AUC: 99.26%, accuracy: 95.53%\n",
      "# testing  | log loss: 64.34%, AUC: 98.13%, accuracy: 89.55%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "68 | Thu Sep 28 00:13:26 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.12796000518967027,\n",
      " 'max_depth': 6,\n",
      " 'max_features': 'sqrt',\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 6,\n",
      " 'subsample': 0.9151543750768408}\n",
      "\n",
      "# training | log loss: 39.91%, AUC: 98.68%, accuracy: 94.17%\n",
      "# testing  | log loss: 43.96%, AUC: 97.42%, accuracy: 90.95%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "69 | Thu Sep 28 00:13:27 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.06662353247986495,\n",
      " 'max_depth': 9,\n",
      " 'max_features': 'log2',\n",
      " 'min_samples_leaf': 5,\n",
      " 'min_samples_split': 3,\n",
      " 'subsample': 0.8267103427398937}\n",
      "\n",
      "# training | log loss: 47.58%, AUC: 99.53%, accuracy: 96.93%\n",
      "# testing  | log loss: 51.00%, AUC: 98.44%, accuracy: 95.78%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "70 | Thu Sep 28 00:13:27 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.06646005427752509,\n",
      " 'max_depth': 9,\n",
      " 'max_features': None,\n",
      " 'min_samples_leaf': 6,\n",
      " 'min_samples_split': 20,\n",
      " 'subsample': 0.9685195558927422}\n",
      "\n",
      "# training | log loss: 46.45%, AUC: 99.61%, accuracy: 97.03%\n",
      "# testing  | log loss: 50.11%, AUC: 98.28%, accuracy: 93.47%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "71 | Thu Sep 28 00:13:27 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.1278271374959585,\n",
      " 'max_depth': 2,\n",
      " 'max_features': 'log2',\n",
      " 'min_samples_leaf': 10,\n",
      " 'min_samples_split': 20,\n",
      " 'subsample': 0.8239165648320492}\n",
      "\n",
      "# training | log loss: 59.00%, AUC: 90.35%, accuracy: 80.63%\n",
      "# testing  | log loss: 63.21%, AUC: 88.35%, accuracy: 74.72%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "72 | Thu Sep 28 00:13:27 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.0636582881651696,\n",
      " 'max_depth': 4,\n",
      " 'max_features': None,\n",
      " 'min_samples_leaf': 8,\n",
      " 'min_samples_split': 5,\n",
      " 'subsample': 0.9509396961036256}\n",
      "\n",
      "# training | log loss: 50.09%, AUC: 97.37%, accuracy: 93.43%\n",
      "# testing  | log loss: 53.72%, AUC: 96.56%, accuracy: 89.14%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "73 | Thu Sep 28 00:13:27 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.11508053185674742,\n",
      " 'max_depth': 6,\n",
      " 'max_features': 'sqrt',\n",
      " 'min_samples_leaf': 7,\n",
      " 'min_samples_split': 10,\n",
      " 'subsample': 0.9209564230317362}\n",
      "\n",
      "# training | log loss: 44.48%, AUC: 97.98%, accuracy: 93.27%\n",
      "# testing  | log loss: 48.18%, AUC: 96.67%, accuracy: 90.93%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "74 | Thu Sep 28 00:13:27 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.011371186253498363,\n",
      " 'max_depth': 4,\n",
      " 'max_features': 'log2',\n",
      " 'min_samples_leaf': 7,\n",
      " 'min_samples_split': 17,\n",
      " 'subsample': 0.8476465245986046}\n",
      "\n",
      "# training | log loss: 65.94%, AUC: 97.31%, accuracy: 63.60%\n",
      "# testing  | log loss: 69.12%, AUC: 96.75%, accuracy: 38.10%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "75 | Thu Sep 28 00:13:27 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.1214300054984113,\n",
      " 'max_depth': 3,\n",
      " 'max_features': 'log2',\n",
      " 'min_samples_leaf': 3,\n",
      " 'min_samples_split': 8,\n",
      " 'subsample': 0.8550329665534532}\n",
      "\n",
      "# training | log loss: 50.65%, AUC: 94.19%, accuracy: 87.43%\n",
      "# testing  | log loss: 55.24%, AUC: 92.23%, accuracy: 84.16%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "76 | Thu Sep 28 00:13:27 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.05753465147861896,\n",
      " 'max_depth': 9,\n",
      " 'max_features': None,\n",
      " 'min_samples_leaf': 2,\n",
      " 'min_samples_split': 15,\n",
      " 'subsample': 0.8644215773278825}\n",
      "\n",
      "# training | log loss: 48.49%, AUC: 99.76%, accuracy: 97.77%\n",
      "# testing  | log loss: 52.16%, AUC: 98.36%, accuracy: 94.31%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "77 | Thu Sep 28 00:13:27 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.08868633262636975,\n",
      " 'max_depth': 6,\n",
      " 'max_features': 'sqrt',\n",
      " 'min_samples_leaf': 2,\n",
      " 'min_samples_split': 12,\n",
      " 'subsample': 0.9970088460191444}\n",
      "\n",
      "# training | log loss: 46.01%, AUC: 98.92%, accuracy: 96.23%\n",
      "# testing  | log loss: 48.95%, AUC: 98.07%, accuracy: 95.69%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "78 | Thu Sep 28 00:13:27 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.05388052616236934,\n",
      " 'max_depth': 6,\n",
      " 'max_features': 'sqrt',\n",
      " 'min_samples_leaf': 6,\n",
      " 'min_samples_split': 14,\n",
      " 'subsample': 0.914203697636145}\n",
      "\n",
      "# training | log loss: 54.15%, AUC: 98.68%, accuracy: 93.73%\n",
      "# testing  | log loss: 57.67%, AUC: 97.64%, accuracy: 90.23%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "79 | Thu Sep 28 00:13:27 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.1971028224793494,\n",
      " 'max_depth': 4,\n",
      " 'max_features': 'log2',\n",
      " 'min_samples_leaf': 7,\n",
      " 'min_samples_split': 11,\n",
      " 'subsample': 0.9075248707002062}\n",
      "\n",
      "# training | log loss: 38.70%, AUC: 97.66%, accuracy: 92.67%\n",
      "# testing  | log loss: 43.63%, AUC: 96.66%, accuracy: 89.21%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "80 | Thu Sep 28 00:13:27 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.08315813922879794,\n",
      " 'max_depth': 5,\n",
      " 'max_features': None,\n",
      " 'min_samples_leaf': 9,\n",
      " 'min_samples_split': 16,\n",
      " 'subsample': 0.82477526663618}\n",
      "\n",
      "# training | log loss: 44.91%, AUC: 98.36%, accuracy: 94.00%\n",
      "# testing  | log loss: 48.45%, AUC: 97.42%, accuracy: 90.40%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "81 | Thu Sep 28 00:13:27 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 5\n",
      "{'learning_rate': 0.09944727476859969,\n",
      " 'max_depth': 3,\n",
      " 'max_features': 'log2',\n",
      " 'min_samples_leaf': 8,\n",
      " 'min_samples_split': 8,\n",
      " 'subsample': 0.9117156530318209}\n",
      "\n",
      "# training | log loss: 55.99%, AUC: 96.06%, accuracy: 91.47%\n",
      "# testing  | log loss: 60.73%, AUC: 93.91%, accuracy: 83.61%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "*** 27.0 configurations x 3.0 iterations each\n",
      "\n",
      "82 | Thu Sep 28 00:13:27 2017 | lowest loss so far: 0.3090 (run 5)\n",
      "\n",
      "n_estimators: 15\n",
      "{'learning_rate': 0.17069686082741595,\n",
      " 'max_depth': 9,\n",
      " 'max_features': None,\n",
      " 'min_samples_leaf': 3,\n",
      " 'min_samples_split': 6,\n",
      " 'subsample': 0.8139185851430316}\n",
      "\n",
      "# training | log loss: 6.77%, AUC: 99.96%, accuracy: 99.37%\n",
      "# testing  | log loss: 13.47%, AUC: 98.73%, accuracy: 96.20%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "83 | Thu Sep 28 00:13:27 2017 | lowest loss so far: 0.1347 (run 82)\n",
      "\n",
      "n_estimators: 15\n",
      "{'learning_rate': 0.1806841170282773,\n",
      " 'max_depth': 10,\n",
      " 'max_features': 'sqrt',\n",
      " 'min_samples_leaf': 6,\n",
      " 'min_samples_split': 9,\n",
      " 'subsample': 0.8191090250789204}\n",
      "\n",
      "# training | log loss: 9.66%, AUC: 99.93%, accuracy: 98.53%\n",
      "# testing  | log loss: 14.31%, AUC: 98.75%, accuracy: 97.28%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "84 | Thu Sep 28 00:13:28 2017 | lowest loss so far: 0.1347 (run 82)\n",
      "\n",
      "n_estimators: 15\n",
      "{'learning_rate': 0.1771437578615261,\n",
      " 'max_depth': 6,\n",
      " 'max_features': None,\n",
      " 'min_samples_leaf': 8,\n",
      " 'min_samples_split': 6,\n",
      " 'subsample': 0.8231313694203439}\n",
      "\n",
      "# training | log loss: 11.16%, AUC: 99.67%, accuracy: 97.27%\n",
      "# testing  | log loss: 15.28%, AUC: 98.68%, accuracy: 95.47%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "85 | Thu Sep 28 00:13:28 2017 | lowest loss so far: 0.1347 (run 82)\n",
      "\n",
      "n_estimators: 15\n",
      "{'learning_rate': 0.1769846620987672,\n",
      " 'max_depth': 9,\n",
      " 'max_features': 'sqrt',\n",
      " 'min_samples_leaf': 4,\n",
      " 'min_samples_split': 9,\n",
      " 'subsample': 0.9725755848387072}\n",
      "\n",
      "# training | log loss: 9.27%, AUC: 99.90%, accuracy: 98.87%\n",
      "# testing  | log loss: 15.12%, AUC: 98.74%, accuracy: 96.67%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "86 | Thu Sep 28 00:13:28 2017 | lowest loss so far: 0.1347 (run 82)\n",
      "\n",
      "n_estimators: 15\n",
      "{'learning_rate': 0.17385220664488754,\n",
      " 'max_depth': 7,\n",
      " 'max_features': 'sqrt',\n",
      " 'min_samples_leaf': 7,\n",
      " 'min_samples_split': 19,\n",
      " 'subsample': 0.8482517645970387}\n",
      "\n",
      "# training | log loss: 13.14%, AUC: 99.62%, accuracy: 97.10%\n",
      "# testing  | log loss: 17.70%, AUC: 98.49%, accuracy: 95.93%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "87 | Thu Sep 28 00:13:28 2017 | lowest loss so far: 0.1347 (run 82)\n",
      "\n",
      "n_estimators: 15\n",
      "{'learning_rate': 0.15042194352721044,\n",
      " 'max_depth': 10,\n",
      " 'max_features': 'log2',\n",
      " 'min_samples_leaf': 5,\n",
      " 'min_samples_split': 7,\n",
      " 'subsample': 0.8774323852562586}\n",
      "\n",
      "# training | log loss: 10.97%, AUC: 99.92%, accuracy: 98.73%\n",
      "# testing  | log loss: 15.97%, AUC: 98.90%, accuracy: 97.14%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "88 | Thu Sep 28 00:13:28 2017 | lowest loss so far: 0.1347 (run 82)\n",
      "\n",
      "n_estimators: 15\n",
      "{'learning_rate': 0.19231464207271945,\n",
      " 'max_depth': 4,\n",
      " 'max_features': 'sqrt',\n",
      " 'min_samples_leaf': 4,\n",
      " 'min_samples_split': 15,\n",
      " 'subsample': 0.9570981494669738}\n",
      "\n",
      "# training | log loss: 19.76%, AUC: 98.73%, accuracy: 95.20%\n",
      "# testing  | log loss: 21.98%, AUC: 98.23%, accuracy: 95.63%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "89 | Thu Sep 28 00:13:28 2017 | lowest loss so far: 0.1347 (run 82)\n",
      "\n",
      "n_estimators: 15\n",
      "{'learning_rate': 0.16680923901424993,\n",
      " 'max_depth': 8,\n",
      " 'max_features': 'log2',\n",
      " 'min_samples_leaf': 8,\n",
      " 'min_samples_split': 14,\n",
      " 'subsample': 0.9039515692026208}\n",
      "\n",
      "# training | log loss: 12.83%, AUC: 99.73%, accuracy: 97.43%\n",
      "# testing  | log loss: 17.41%, AUC: 98.59%, accuracy: 96.12%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "90 | Thu Sep 28 00:13:28 2017 | lowest loss so far: 0.1347 (run 82)\n",
      "\n",
      "n_estimators: 15\n",
      "{'learning_rate': 0.15485009197981445,\n",
      " 'max_depth': 10,\n",
      " 'max_features': 'log2',\n",
      " 'min_samples_leaf': 6,\n",
      " 'min_samples_split': 11,\n",
      " 'subsample': 0.8229184707116074}\n",
      "\n",
      "# training | log loss: 11.19%, AUC: 99.85%, accuracy: 98.60%\n",
      "# testing  | log loss: 16.55%, AUC: 98.66%, accuracy: 96.44%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "91 | Thu Sep 28 00:13:29 2017 | lowest loss so far: 0.1347 (run 82)\n",
      "\n",
      "n_estimators: 15\n",
      "{'learning_rate': 0.18435730545244297,\n",
      " 'max_depth': 5,\n",
      " 'max_features': 'log2',\n",
      " 'min_samples_leaf': 7,\n",
      " 'min_samples_split': 19,\n",
      " 'subsample': 0.9724838814069894}\n",
      "\n",
      "# training | log loss: 18.64%, AUC: 99.16%, accuracy: 95.33%\n",
      "# testing  | log loss: 24.18%, AUC: 98.10%, accuracy: 93.28%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "92 | Thu Sep 28 00:13:29 2017 | lowest loss so far: 0.1347 (run 82)\n",
      "\n",
      "n_estimators: 15\n",
      "{'learning_rate': 0.13987247521336282,\n",
      " 'max_depth': 8,\n",
      " 'max_features': 'log2',\n",
      " 'min_samples_leaf': 7,\n",
      " 'min_samples_split': 4,\n",
      " 'subsample': 0.9389281611550853}\n",
      "\n",
      "# training | log loss: 16.61%, AUC: 99.62%, accuracy: 96.90%\n",
      "# testing  | log loss: 21.46%, AUC: 98.46%, accuracy: 95.45%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "93 | Thu Sep 28 00:13:29 2017 | lowest loss so far: 0.1347 (run 82)\n",
      "\n",
      "n_estimators: 15\n",
      "{'learning_rate': 0.1252669037490443,\n",
      " 'max_depth': 10,\n",
      " 'max_features': 'sqrt',\n",
      " 'min_samples_leaf': 8,\n",
      " 'min_samples_split': 5,\n",
      " 'subsample': 0.9046079334166649}\n",
      "\n",
      "# training | log loss: 15.37%, AUC: 99.77%, accuracy: 97.83%\n",
      "# testing  | log loss: 19.87%, AUC: 98.63%, accuracy: 96.51%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "94 | Thu Sep 28 00:13:29 2017 | lowest loss so far: 0.1347 (run 82)\n",
      "\n",
      "n_estimators: 15\n",
      "{'learning_rate': 0.1481604664666325,\n",
      " 'max_depth': 7,\n",
      " 'max_features': 'sqrt',\n",
      " 'min_samples_leaf': 2,\n",
      " 'min_samples_split': 15,\n",
      " 'subsample': 0.8638638008105615}\n",
      "\n",
      "# training | log loss: 13.49%, AUC: 99.67%, accuracy: 97.83%\n",
      "# testing  | log loss: 17.15%, AUC: 98.77%, accuracy: 97.03%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "95 | Thu Sep 28 00:13:29 2017 | lowest loss so far: 0.1347 (run 82)\n",
      "\n",
      "n_estimators: 15\n",
      "{'learning_rate': 0.18861368617982344,\n",
      " 'max_depth': 5,\n",
      " 'max_features': 'sqrt',\n",
      " 'min_samples_leaf': 9,\n",
      " 'min_samples_split': 11,\n",
      " 'subsample': 0.8733090008355578}\n",
      "\n",
      "# training | log loss: 17.89%, AUC: 99.03%, accuracy: 95.53%\n",
      "# testing  | log loss: 22.06%, AUC: 98.03%, accuracy: 94.71%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "96 | Thu Sep 28 00:13:29 2017 | lowest loss so far: 0.1347 (run 82)\n",
      "\n",
      "n_estimators: 15\n",
      "{'learning_rate': 0.14323745355368028,\n",
      " 'max_depth': 7,\n",
      " 'max_features': 'sqrt',\n",
      " 'min_samples_leaf': 2,\n",
      " 'min_samples_split': 19,\n",
      " 'subsample': 0.9789507029177924}\n",
      "\n",
      "# training | log loss: 15.48%, AUC: 99.56%, accuracy: 97.10%\n",
      "# testing  | log loss: 19.24%, AUC: 98.65%, accuracy: 96.25%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "97 | Thu Sep 28 00:13:29 2017 | lowest loss so far: 0.1347 (run 82)\n",
      "\n",
      "n_estimators: 15\n",
      "{'learning_rate': 0.15583855270255634,\n",
      " 'max_depth': 3,\n",
      " 'max_features': None,\n",
      " 'min_samples_leaf': 3,\n",
      " 'min_samples_split': 10,\n",
      " 'subsample': 0.9193392775510302}\n",
      "\n",
      "# training | log loss: 20.61%, AUC: 98.53%, accuracy: 94.00%\n",
      "# testing  | log loss: 23.16%, AUC: 98.07%, accuracy: 91.62%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "98 | Thu Sep 28 00:13:29 2017 | lowest loss so far: 0.1347 (run 82)\n",
      "\n",
      "n_estimators: 15\n",
      "{'learning_rate': 0.15609784942438312,\n",
      " 'max_depth': 3,\n",
      " 'max_features': None,\n",
      " 'min_samples_leaf': 8,\n",
      " 'min_samples_split': 19,\n",
      " 'subsample': 0.9458587456887256}\n",
      "\n",
      "# training | log loss: 20.00%, AUC: 98.37%, accuracy: 93.83%\n",
      "# testing  | log loss: 21.95%, AUC: 97.88%, accuracy: 91.87%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "99 | Thu Sep 28 00:13:29 2017 | lowest loss so far: 0.1347 (run 82)\n",
      "\n",
      "n_estimators: 15\n",
      "{'learning_rate': 0.1460354004351075,\n",
      " 'max_depth': 6,\n",
      " 'max_features': 'log2',\n",
      " 'min_samples_leaf': 4,\n",
      " 'min_samples_split': 4,\n",
      " 'subsample': 0.9848457686458227}\n",
      "\n",
      "# training | log loss: 17.52%, AUC: 99.37%, accuracy: 96.37%\n",
      "# testing  | log loss: 22.00%, AUC: 98.45%, accuracy: 95.49%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "100 | Thu Sep 28 00:13:30 2017 | lowest loss so far: 0.1347 (run 82)\n",
      "\n",
      "n_estimators: 15\n",
      "{'learning_rate': 0.10434608771666079,\n",
      " 'max_depth': 10,\n",
      " 'max_features': None,\n",
      " 'min_samples_leaf': 6,\n",
      " 'min_samples_split': 9,\n",
      " 'subsample': 0.8194257546806554}\n",
      "\n",
      "# training | log loss: 14.97%, AUC: 99.91%, accuracy: 98.43%\n",
      "# testing  | log loss: 20.12%, AUC: 98.66%, accuracy: 95.79%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "101 | Thu Sep 28 00:13:30 2017 | lowest loss so far: 0.1347 (run 82)\n",
      "\n",
      "n_estimators: 15\n",
      "{'learning_rate': 0.19398846486298116,\n",
      " 'max_depth': 5,\n",
      " 'max_features': 'sqrt',\n",
      " 'min_samples_leaf': 2,\n",
      " 'min_samples_split': 16,\n",
      " 'subsample': 0.8796724559223199}\n",
      "\n",
      "# training | log loss: 14.54%, AUC: 99.35%, accuracy: 96.03%\n",
      "# testing  | log loss: 18.00%, AUC: 98.54%, accuracy: 95.44%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "102 | Thu Sep 28 00:13:30 2017 | lowest loss so far: 0.1347 (run 82)\n",
      "\n",
      "n_estimators: 15\n",
      "{'learning_rate': 0.1185457477381868,\n",
      " 'max_depth': 10,\n",
      " 'max_features': 'sqrt',\n",
      " 'min_samples_leaf': 10,\n",
      " 'min_samples_split': 3,\n",
      " 'subsample': 0.8755575296420445}\n",
      "\n",
      "# training | log loss: 16.63%, AUC: 99.78%, accuracy: 97.10%\n",
      "# testing  | log loss: 21.29%, AUC: 98.59%, accuracy: 96.55%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "103 | Thu Sep 28 00:13:30 2017 | lowest loss so far: 0.1347 (run 82)\n",
      "\n",
      "n_estimators: 15\n",
      "{'learning_rate': 0.1066733053310453,\n",
      " 'max_depth': 6,\n",
      " 'max_features': None,\n",
      " 'min_samples_leaf': 3,\n",
      " 'min_samples_split': 10,\n",
      " 'subsample': 0.9959838070161853}\n",
      "\n",
      "# training | log loss: 17.39%, AUC: 99.47%, accuracy: 96.97%\n",
      "# testing  | log loss: 21.51%, AUC: 98.53%, accuracy: 95.27%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "104 | Thu Sep 28 00:13:30 2017 | lowest loss so far: 0.1347 (run 82)\n",
      "\n",
      "n_estimators: 15\n",
      "{'learning_rate': 0.10215479632853298,\n",
      " 'max_depth': 8,\n",
      " 'max_features': None,\n",
      " 'min_samples_leaf': 7,\n",
      " 'min_samples_split': 17,\n",
      " 'subsample': 0.975396683453934}\n",
      "\n",
      "# training | log loss: 16.81%, AUC: 99.75%, accuracy: 97.70%\n",
      "# testing  | log loss: 21.11%, AUC: 98.57%, accuracy: 95.81%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "105 | Thu Sep 28 00:13:30 2017 | lowest loss so far: 0.1347 (run 82)\n",
      "\n",
      "n_estimators: 15\n",
      "{'learning_rate': 0.10289948874412463,\n",
      " 'max_depth': 9,\n",
      " 'max_features': None,\n",
      " 'min_samples_leaf': 10,\n",
      " 'min_samples_split': 15,\n",
      " 'subsample': 0.9346719632545584}\n",
      "\n",
      "# training | log loss: 16.74%, AUC: 99.79%, accuracy: 97.67%\n",
      "# testing  | log loss: 21.13%, AUC: 98.55%, accuracy: 96.31%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "106 | Thu Sep 28 00:13:31 2017 | lowest loss so far: 0.1347 (run 82)\n",
      "\n",
      "n_estimators: 15\n",
      "{'learning_rate': 0.11878790593045495,\n",
      " 'max_depth': 6,\n",
      " 'max_features': 'sqrt',\n",
      " 'min_samples_leaf': 6,\n",
      " 'min_samples_split': 11,\n",
      " 'subsample': 0.8045936798307327}\n",
      "\n",
      "# training | log loss: 21.02%, AUC: 99.18%, accuracy: 95.93%\n",
      "# testing  | log loss: 23.87%, AUC: 98.24%, accuracy: 95.88%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "107 | Thu Sep 28 00:13:31 2017 | lowest loss so far: 0.1347 (run 82)\n",
      "\n",
      "n_estimators: 15\n",
      "{'learning_rate': 0.11416623794748747,\n",
      " 'max_depth': 9,\n",
      " 'max_features': 'log2',\n",
      " 'min_samples_leaf': 10,\n",
      " 'min_samples_split': 19,\n",
      " 'subsample': 0.8365481052945872}\n",
      "\n",
      "# training | log loss: 18.28%, AUC: 99.62%, accuracy: 96.73%\n",
      "# testing  | log loss: 22.39%, AUC: 98.62%, accuracy: 96.17%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "108 | Thu Sep 28 00:13:31 2017 | lowest loss so far: 0.1347 (run 82)\n",
      "\n",
      "n_estimators: 15\n",
      "{'learning_rate': 0.11497583866607342,\n",
      " 'max_depth': 6,\n",
      " 'max_features': 'sqrt',\n",
      " 'min_samples_leaf': 3,\n",
      " 'min_samples_split': 9,\n",
      " 'subsample': 0.8261043653430352}\n",
      "\n",
      "# training | log loss: 20.20%, AUC: 99.41%, accuracy: 96.67%\n",
      "# testing  | log loss: 23.62%, AUC: 98.44%, accuracy: 96.08%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "*** 9.0 configurations x 9.0 iterations each\n",
      "\n",
      "109 | Thu Sep 28 00:13:31 2017 | lowest loss so far: 0.1347 (run 82)\n",
      "\n",
      "n_estimators: 45\n",
      "{'learning_rate': 0.17069686082741595,\n",
      " 'max_depth': 9,\n",
      " 'max_features': None,\n",
      " 'min_samples_leaf': 3,\n",
      " 'min_samples_split': 6,\n",
      " 'subsample': 0.8139185851430316}\n",
      "\n",
      "# training | log loss: 0.47%, AUC: 100.00%, accuracy: 100.00%\n",
      "# testing  | log loss: 8.64%, AUC: 98.97%, accuracy: 97.08%\n",
      "\n",
      "1 seconds.\n",
      "\n",
      "110 | Thu Sep 28 00:13:32 2017 | lowest loss so far: 0.0864 (run 109)\n",
      "\n",
      "n_estimators: 45\n",
      "{'learning_rate': 0.1806841170282773,\n",
      " 'max_depth': 10,\n",
      " 'max_features': 'sqrt',\n",
      " 'min_samples_leaf': 6,\n",
      " 'min_samples_split': 9,\n",
      " 'subsample': 0.8191090250789204}\n",
      "\n",
      "# training | log loss: 1.55%, AUC: 100.00%, accuracy: 99.93%\n",
      "# testing  | log loss: 9.27%, AUC: 99.01%, accuracy: 96.88%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "111 | Thu Sep 28 00:13:32 2017 | lowest loss so far: 0.0864 (run 109)\n",
      "\n",
      "n_estimators: 45\n",
      "{'learning_rate': 0.1769846620987672,\n",
      " 'max_depth': 9,\n",
      " 'max_features': 'sqrt',\n",
      " 'min_samples_leaf': 4,\n",
      " 'min_samples_split': 9,\n",
      " 'subsample': 0.9725755848387072}\n",
      "\n",
      "# training | log loss: 1.43%, AUC: 100.00%, accuracy: 99.93%\n",
      "# testing  | log loss: 8.61%, AUC: 99.00%, accuracy: 97.19%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "112 | Thu Sep 28 00:13:33 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 45\n",
      "{'learning_rate': 0.1771437578615261,\n",
      " 'max_depth': 6,\n",
      " 'max_features': None,\n",
      " 'min_samples_leaf': 8,\n",
      " 'min_samples_split': 6,\n",
      " 'subsample': 0.8231313694203439}\n",
      "\n",
      "# training | log loss: 3.31%, AUC: 99.99%, accuracy: 99.40%\n",
      "# testing  | log loss: 10.68%, AUC: 98.98%, accuracy: 96.25%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "113 | Thu Sep 28 00:13:33 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 45\n",
      "{'learning_rate': 0.15042194352721044,\n",
      " 'max_depth': 10,\n",
      " 'max_features': 'log2',\n",
      " 'min_samples_leaf': 5,\n",
      " 'min_samples_split': 7,\n",
      " 'subsample': 0.8774323852562586}\n",
      "\n",
      "# training | log loss: 2.02%, AUC: 100.00%, accuracy: 99.87%\n",
      "# testing  | log loss: 9.51%, AUC: 99.01%, accuracy: 97.03%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "114 | Thu Sep 28 00:13:34 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 45\n",
      "{'learning_rate': 0.15485009197981445,\n",
      " 'max_depth': 10,\n",
      " 'max_features': 'log2',\n",
      " 'min_samples_leaf': 6,\n",
      " 'min_samples_split': 11,\n",
      " 'subsample': 0.8229184707116074}\n",
      "\n",
      "# training | log loss: 2.02%, AUC: 100.00%, accuracy: 99.90%\n",
      "# testing  | log loss: 9.52%, AUC: 98.93%, accuracy: 97.04%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "115 | Thu Sep 28 00:13:34 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 45\n",
      "{'learning_rate': 0.1481604664666325,\n",
      " 'max_depth': 7,\n",
      " 'max_features': 'sqrt',\n",
      " 'min_samples_leaf': 2,\n",
      " 'min_samples_split': 15,\n",
      " 'subsample': 0.8638638008105615}\n",
      "\n",
      "# training | log loss: 4.16%, AUC: 99.98%, accuracy: 99.33%\n",
      "# testing  | log loss: 11.19%, AUC: 98.95%, accuracy: 96.59%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "116 | Thu Sep 28 00:13:34 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 45\n",
      "{'learning_rate': 0.16680923901424993,\n",
      " 'max_depth': 8,\n",
      " 'max_features': 'log2',\n",
      " 'min_samples_leaf': 8,\n",
      " 'min_samples_split': 14,\n",
      " 'subsample': 0.9039515692026208}\n",
      "\n",
      "# training | log loss: 3.65%, AUC: 99.99%, accuracy: 99.47%\n",
      "# testing  | log loss: 10.97%, AUC: 98.93%, accuracy: 96.48%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "117 | Thu Sep 28 00:13:35 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 45\n",
      "{'learning_rate': 0.17385220664488754,\n",
      " 'max_depth': 7,\n",
      " 'max_features': 'sqrt',\n",
      " 'min_samples_leaf': 7,\n",
      " 'min_samples_split': 19,\n",
      " 'subsample': 0.8482517645970387}\n",
      "\n",
      "# training | log loss: 4.81%, AUC: 99.94%, accuracy: 98.87%\n",
      "# testing  | log loss: 11.97%, AUC: 98.87%, accuracy: 96.01%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "*** 3.0 configurations x 27.0 iterations each\n",
      "\n",
      "118 | Thu Sep 28 00:13:35 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 135\n",
      "{'learning_rate': 0.1769846620987672,\n",
      " 'max_depth': 9,\n",
      " 'max_features': 'sqrt',\n",
      " 'min_samples_leaf': 4,\n",
      " 'min_samples_split': 9,\n",
      " 'subsample': 0.9725755848387072}\n",
      "\n",
      "# training | log loss: 0.01%, AUC: 100.00%, accuracy: 100.00%\n",
      "# testing  | log loss: 10.62%, AUC: 98.99%, accuracy: 97.13%\n",
      "\n",
      "1 seconds.\n",
      "\n",
      "119 | Thu Sep 28 00:13:36 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 135\n",
      "{'learning_rate': 0.17069686082741595,\n",
      " 'max_depth': 9,\n",
      " 'max_features': None,\n",
      " 'min_samples_leaf': 3,\n",
      " 'min_samples_split': 6,\n",
      " 'subsample': 0.8139185851430316}\n",
      "\n",
      "# training | log loss: 0.01%, AUC: 100.00%, accuracy: 100.00%\n",
      "# testing  | log loss: 10.39%, AUC: 98.98%, accuracy: 97.34%\n",
      "\n",
      "2 seconds.\n",
      "\n",
      "120 | Thu Sep 28 00:13:38 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 135\n",
      "{'learning_rate': 0.1806841170282773,\n",
      " 'max_depth': 10,\n",
      " 'max_features': 'sqrt',\n",
      " 'min_samples_leaf': 6,\n",
      " 'min_samples_split': 9,\n",
      " 'subsample': 0.8191090250789204}\n",
      "\n",
      "# training | log loss: 0.01%, AUC: 100.00%, accuracy: 100.00%\n",
      "# testing  | log loss: 10.44%, AUC: 99.00%, accuracy: 97.19%\n",
      "\n",
      "1 seconds.\n",
      "\n",
      "*** 1.0 configurations x 81.0 iterations each\n",
      "\n",
      "121 | Thu Sep 28 00:13:39 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 405\n",
      "{'learning_rate': 0.17069686082741595,\n",
      " 'max_depth': 9,\n",
      " 'max_features': None,\n",
      " 'min_samples_leaf': 3,\n",
      " 'min_samples_split': 6,\n",
      " 'subsample': 0.8139185851430316}\n",
      "\n",
      "# training | log loss: 0.01%, AUC: 100.00%, accuracy: 100.00%\n",
      "# testing  | log loss: 10.12%, AUC: 99.00%, accuracy: 97.31%\n",
      "\n",
      "2 seconds.\n",
      "\n",
      "*** 27 configurations x 3.0 iterations each\n",
      "\n",
      "122 | Thu Sep 28 00:13:41 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 15\n",
      "{'learning_rate': 0.11736313453730748,\n",
      " 'max_depth': 8,\n",
      " 'max_features': None,\n",
      " 'min_samples_leaf': 9,\n",
      " 'min_samples_split': 13,\n",
      " 'subsample': 0.971854664482022}\n",
      "\n",
      "# training | log loss: 15.09%, AUC: 99.74%, accuracy: 97.50%\n",
      "# testing  | log loss: 19.70%, AUC: 98.56%, accuracy: 95.49%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "123 | Thu Sep 28 00:13:41 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 15\n",
      "{'learning_rate': 0.014390640266480993,\n",
      " 'max_depth': 9,\n",
      " 'max_features': 'sqrt',\n",
      " 'min_samples_leaf': 2,\n",
      " 'min_samples_split': 11,\n",
      " 'subsample': 0.9136969680504571}\n",
      "\n",
      "# training | log loss: 54.56%, AUC: 99.72%, accuracy: 97.53%\n",
      "# testing  | log loss: 58.15%, AUC: 98.76%, accuracy: 95.62%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "124 | Thu Sep 28 00:13:41 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 15\n",
      "{'learning_rate': 0.19116990619710336,\n",
      " 'max_depth': 7,\n",
      " 'max_features': 'log2',\n",
      " 'min_samples_leaf': 8,\n",
      " 'min_samples_split': 10,\n",
      " 'subsample': 0.8525005098343338}\n",
      "\n",
      "# training | log loss: 12.11%, AUC: 99.69%, accuracy: 97.10%\n",
      "# testing  | log loss: 16.16%, AUC: 98.70%, accuracy: 96.76%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "125 | Thu Sep 28 00:13:42 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 15\n",
      "{'learning_rate': 0.035019808105702106,\n",
      " 'max_depth': 10,\n",
      " 'max_features': None,\n",
      " 'min_samples_leaf': 3,\n",
      " 'min_samples_split': 5,\n",
      " 'subsample': 0.8545529484684362}\n",
      "\n",
      "# training | log loss: 36.68%, AUC: 99.94%, accuracy: 98.90%\n",
      "# testing  | log loss: 41.24%, AUC: 98.33%, accuracy: 94.99%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "126 | Thu Sep 28 00:13:42 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 15\n",
      "{'learning_rate': 0.0906700945101033,\n",
      " 'max_depth': 9,\n",
      " 'max_features': None,\n",
      " 'min_samples_leaf': 7,\n",
      " 'min_samples_split': 17,\n",
      " 'subsample': 0.8542743881527751}\n",
      "\n",
      "# training | log loss: 18.59%, AUC: 99.79%, accuracy: 97.57%\n",
      "# testing  | log loss: 22.91%, AUC: 98.69%, accuracy: 95.29%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "127 | Thu Sep 28 00:13:42 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 15\n",
      "{'learning_rate': 0.17677822238193847,\n",
      " 'max_depth': 9,\n",
      " 'max_features': None,\n",
      " 'min_samples_leaf': 9,\n",
      " 'min_samples_split': 14,\n",
      " 'subsample': 0.8676310595202422}\n",
      "\n",
      "# training | log loss: 8.20%, AUC: 99.92%, accuracy: 98.80%\n",
      "# testing  | log loss: 13.68%, AUC: 98.82%, accuracy: 96.58%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "128 | Thu Sep 28 00:13:42 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 15\n",
      "{'learning_rate': 0.06539113803674845,\n",
      " 'max_depth': 7,\n",
      " 'max_features': 'sqrt',\n",
      " 'min_samples_leaf': 7,\n",
      " 'min_samples_split': 14,\n",
      " 'subsample': 0.8353298955143321}\n",
      "\n",
      "# training | log loss: 31.02%, AUC: 99.13%, accuracy: 95.63%\n",
      "# testing  | log loss: 34.30%, AUC: 98.24%, accuracy: 95.86%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "129 | Thu Sep 28 00:13:42 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 15\n",
      "{'learning_rate': 0.036172412107512576,\n",
      " 'max_depth': 6,\n",
      " 'max_features': 'sqrt',\n",
      " 'min_samples_leaf': 6,\n",
      " 'min_samples_split': 4,\n",
      " 'subsample': 0.9663595335664207}\n",
      "\n",
      "# training | log loss: 46.81%, AUC: 98.74%, accuracy: 94.60%\n",
      "# testing  | log loss: 51.91%, AUC: 97.64%, accuracy: 90.27%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "130 | Thu Sep 28 00:13:42 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 15\n",
      "{'learning_rate': 0.07085305837098604,\n",
      " 'max_depth': 8,\n",
      " 'max_features': 'log2',\n",
      " 'min_samples_leaf': 3,\n",
      " 'min_samples_split': 18,\n",
      " 'subsample': 0.8584706290447147}\n",
      "\n",
      "# training | log loss: 26.85%, AUC: 99.59%, accuracy: 96.87%\n",
      "# testing  | log loss: 29.82%, AUC: 98.63%, accuracy: 97.00%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "131 | Thu Sep 28 00:13:43 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 15\n",
      "{'learning_rate': 0.05140346694814492,\n",
      " 'max_depth': 8,\n",
      " 'max_features': 'log2',\n",
      " 'min_samples_leaf': 10,\n",
      " 'min_samples_split': 11,\n",
      " 'subsample': 0.8923797586857144}\n",
      "\n",
      "# training | log loss: 34.45%, AUC: 99.39%, accuracy: 95.80%\n",
      "# testing  | log loss: 37.23%, AUC: 98.54%, accuracy: 96.73%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "132 | Thu Sep 28 00:13:43 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 15\n",
      "{'learning_rate': 0.15853276616681117,\n",
      " 'max_depth': 4,\n",
      " 'max_features': 'sqrt',\n",
      " 'min_samples_leaf': 6,\n",
      " 'min_samples_split': 12,\n",
      " 'subsample': 0.8834939266232357}\n",
      "\n",
      "# training | log loss: 25.04%, AUC: 98.31%, accuracy: 94.30%\n",
      "# testing  | log loss: 29.25%, AUC: 97.55%, accuracy: 92.67%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "133 | Thu Sep 28 00:13:43 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 15\n",
      "{'learning_rate': 0.13052201313739414,\n",
      " 'max_depth': 5,\n",
      " 'max_features': 'log2',\n",
      " 'min_samples_leaf': 4,\n",
      " 'min_samples_split': 10,\n",
      " 'subsample': 0.8190087352266454}\n",
      "\n",
      "# training | log loss: 24.35%, AUC: 98.91%, accuracy: 95.10%\n",
      "# testing  | log loss: 28.03%, AUC: 97.90%, accuracy: 93.21%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "134 | Thu Sep 28 00:13:43 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 15\n",
      "{'learning_rate': 0.019208225900422274,\n",
      " 'max_depth': 3,\n",
      " 'max_features': 'log2',\n",
      " 'min_samples_leaf': 7,\n",
      " 'min_samples_split': 11,\n",
      " 'subsample': 0.8976564641184354}\n",
      "\n",
      "# training | log loss: 60.03%, AUC: 96.01%, accuracy: 87.17%\n",
      "# testing  | log loss: 64.00%, AUC: 93.79%, accuracy: 79.47%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "135 | Thu Sep 28 00:13:43 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 15\n",
      "{'learning_rate': 0.09830952699550209,\n",
      " 'max_depth': 10,\n",
      " 'max_features': None,\n",
      " 'min_samples_leaf': 9,\n",
      " 'min_samples_split': 6,\n",
      " 'subsample': 0.858192481778386}\n",
      "\n",
      "# training | log loss: 17.28%, AUC: 99.82%, accuracy: 97.30%\n",
      "# testing  | log loss: 21.64%, AUC: 98.64%, accuracy: 96.22%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "136 | Thu Sep 28 00:13:43 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 15\n",
      "{'learning_rate': 0.15762526041648256,\n",
      " 'max_depth': 7,\n",
      " 'max_features': None,\n",
      " 'min_samples_leaf': 8,\n",
      " 'min_samples_split': 20,\n",
      " 'subsample': 0.883756112950388}\n",
      "\n",
      "# training | log loss: 11.27%, AUC: 99.76%, accuracy: 97.80%\n",
      "# testing  | log loss: 16.03%, AUC: 98.71%, accuracy: 95.52%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "137 | Thu Sep 28 00:13:43 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 15\n",
      "{'learning_rate': 0.11434410905390581,\n",
      " 'max_depth': 6,\n",
      " 'max_features': 'sqrt',\n",
      " 'min_samples_leaf': 10,\n",
      " 'min_samples_split': 19,\n",
      " 'subsample': 0.8756146563921536}\n",
      "\n",
      "# training | log loss: 21.84%, AUC: 99.03%, accuracy: 95.30%\n",
      "# testing  | log loss: 24.41%, AUC: 98.27%, accuracy: 95.58%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "138 | Thu Sep 28 00:13:43 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 15\n",
      "{'learning_rate': 0.19647690528775552,\n",
      " 'max_depth': 8,\n",
      " 'max_features': 'log2',\n",
      " 'min_samples_leaf': 4,\n",
      " 'min_samples_split': 19,\n",
      " 'subsample': 0.802855622382238}\n",
      "\n",
      "# training | log loss: 10.56%, AUC: 99.79%, accuracy: 97.47%\n",
      "# testing  | log loss: 16.13%, AUC: 98.66%, accuracy: 96.08%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "139 | Thu Sep 28 00:13:44 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 15\n",
      "{'learning_rate': 0.1034983368208853,\n",
      " 'max_depth': 5,\n",
      " 'max_features': 'log2',\n",
      " 'min_samples_leaf': 7,\n",
      " 'min_samples_split': 20,\n",
      " 'subsample': 0.8215671988844168}\n",
      "\n",
      "# training | log loss: 25.25%, AUC: 98.75%, accuracy: 95.53%\n",
      "# testing  | log loss: 27.23%, AUC: 98.25%, accuracy: 95.03%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "140 | Thu Sep 28 00:13:44 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 15\n",
      "{'learning_rate': 0.05794617135564509,\n",
      " 'max_depth': 7,\n",
      " 'max_features': None,\n",
      " 'min_samples_leaf': 7,\n",
      " 'min_samples_split': 14,\n",
      " 'subsample': 0.9660883781216264}\n",
      "\n",
      "# training | log loss: 28.64%, AUC: 99.50%, accuracy: 97.10%\n",
      "# testing  | log loss: 32.56%, AUC: 98.35%, accuracy: 93.91%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "141 | Thu Sep 28 00:13:44 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 15\n",
      "{'learning_rate': 0.032453639461080416,\n",
      " 'max_depth': 6,\n",
      " 'max_features': None,\n",
      " 'min_samples_leaf': 4,\n",
      " 'min_samples_split': 3,\n",
      " 'subsample': 0.9264488061324642}\n",
      "\n",
      "# training | log loss: 40.67%, AUC: 99.17%, accuracy: 95.67%\n",
      "# testing  | log loss: 44.37%, AUC: 98.15%, accuracy: 92.20%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "142 | Thu Sep 28 00:13:44 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 15\n",
      "{'learning_rate': 0.09374498319162218,\n",
      " 'max_depth': 3,\n",
      " 'max_features': None,\n",
      " 'min_samples_leaf': 5,\n",
      " 'min_samples_split': 10,\n",
      " 'subsample': 0.8135639351525629}\n",
      "\n",
      "# training | log loss: 26.89%, AUC: 98.00%, accuracy: 93.73%\n",
      "# testing  | log loss: 28.39%, AUC: 97.74%, accuracy: 91.53%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "143 | Thu Sep 28 00:13:44 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 15\n",
      "{'learning_rate': 0.06858390252261033,\n",
      " 'max_depth': 6,\n",
      " 'max_features': 'sqrt',\n",
      " 'min_samples_leaf': 3,\n",
      " 'min_samples_split': 6,\n",
      " 'subsample': 0.876162882081585}\n",
      "\n",
      "# training | log loss: 30.19%, AUC: 99.37%, accuracy: 96.37%\n",
      "# testing  | log loss: 33.79%, AUC: 98.45%, accuracy: 95.43%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "144 | Thu Sep 28 00:13:44 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 15\n",
      "{'learning_rate': 0.10414427810083633,\n",
      " 'max_depth': 6,\n",
      " 'max_features': 'log2',\n",
      " 'min_samples_leaf': 6,\n",
      " 'min_samples_split': 13,\n",
      " 'subsample': 0.8129692016106584}\n",
      "\n",
      "# training | log loss: 23.31%, AUC: 99.14%, accuracy: 95.37%\n",
      "# testing  | log loss: 26.24%, AUC: 98.18%, accuracy: 95.28%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "145 | Thu Sep 28 00:13:44 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 15\n",
      "{'learning_rate': 0.19255277922162595,\n",
      " 'max_depth': 4,\n",
      " 'max_features': None,\n",
      " 'min_samples_leaf': 8,\n",
      " 'min_samples_split': 15,\n",
      " 'subsample': 0.9224934921181591}\n",
      "\n",
      "# training | log loss: 14.18%, AUC: 99.28%, accuracy: 95.40%\n",
      "# testing  | log loss: 17.46%, AUC: 98.52%, accuracy: 93.71%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "146 | Thu Sep 28 00:13:44 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 15\n",
      "{'learning_rate': 0.06324810731929152,\n",
      " 'max_depth': 8,\n",
      " 'max_features': 'log2',\n",
      " 'min_samples_leaf': 4,\n",
      " 'min_samples_split': 3,\n",
      " 'subsample': 0.9763103757990954}\n",
      "\n",
      "# training | log loss: 28.51%, AUC: 99.70%, accuracy: 97.67%\n",
      "# testing  | log loss: 32.00%, AUC: 98.66%, accuracy: 97.04%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "147 | Thu Sep 28 00:13:45 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 15\n",
      "{'learning_rate': 0.06836428110856868,\n",
      " 'max_depth': 8,\n",
      " 'max_features': 'log2',\n",
      " 'min_samples_leaf': 4,\n",
      " 'min_samples_split': 7,\n",
      " 'subsample': 0.8283634550194955}\n",
      "\n",
      "# training | log loss: 28.27%, AUC: 99.53%, accuracy: 96.63%\n",
      "# testing  | log loss: 32.10%, AUC: 98.48%, accuracy: 96.92%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "148 | Thu Sep 28 00:13:45 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 15\n",
      "{'learning_rate': 0.08353884543961744,\n",
      " 'max_depth': 4,\n",
      " 'max_features': 'sqrt',\n",
      " 'min_samples_leaf': 7,\n",
      " 'min_samples_split': 16,\n",
      " 'subsample': 0.8497288912715881}\n",
      "\n",
      "# training | log loss: 34.26%, AUC: 98.17%, accuracy: 93.63%\n",
      "# testing  | log loss: 37.80%, AUC: 97.47%, accuracy: 92.62%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "*** 9.0 configurations x 9.0 iterations each\n",
      "\n",
      "149 | Thu Sep 28 00:13:45 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 45\n",
      "{'learning_rate': 0.17677822238193847,\n",
      " 'max_depth': 9,\n",
      " 'max_features': None,\n",
      " 'min_samples_leaf': 9,\n",
      " 'min_samples_split': 14,\n",
      " 'subsample': 0.8676310595202422}\n",
      "\n",
      "# training | log loss: 1.01%, AUC: 100.00%, accuracy: 100.00%\n",
      "# testing  | log loss: 8.92%, AUC: 98.98%, accuracy: 96.97%\n",
      "\n",
      "1 seconds.\n",
      "\n",
      "150 | Thu Sep 28 00:13:45 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 45\n",
      "{'learning_rate': 0.15762526041648256,\n",
      " 'max_depth': 7,\n",
      " 'max_features': None,\n",
      " 'min_samples_leaf': 8,\n",
      " 'min_samples_split': 20,\n",
      " 'subsample': 0.883756112950388}\n",
      "\n",
      "# training | log loss: 2.51%, AUC: 100.00%, accuracy: 99.80%\n",
      "# testing  | log loss: 9.87%, AUC: 99.07%, accuracy: 96.67%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "151 | Thu Sep 28 00:13:46 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 45\n",
      "{'learning_rate': 0.19647690528775552,\n",
      " 'max_depth': 8,\n",
      " 'max_features': 'log2',\n",
      " 'min_samples_leaf': 4,\n",
      " 'min_samples_split': 19,\n",
      " 'subsample': 0.802855622382238}\n",
      "\n",
      "# training | log loss: 2.40%, AUC: 100.00%, accuracy: 99.77%\n",
      "# testing  | log loss: 9.92%, AUC: 99.02%, accuracy: 96.86%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "152 | Thu Sep 28 00:13:46 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 45\n",
      "{'learning_rate': 0.19116990619710336,\n",
      " 'max_depth': 7,\n",
      " 'max_features': 'log2',\n",
      " 'min_samples_leaf': 8,\n",
      " 'min_samples_split': 10,\n",
      " 'subsample': 0.8525005098343338}\n",
      "\n",
      "# training | log loss: 3.79%, AUC: 99.98%, accuracy: 99.40%\n",
      "# testing  | log loss: 11.49%, AUC: 98.97%, accuracy: 96.12%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "153 | Thu Sep 28 00:13:47 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 45\n",
      "{'learning_rate': 0.19255277922162595,\n",
      " 'max_depth': 4,\n",
      " 'max_features': None,\n",
      " 'min_samples_leaf': 8,\n",
      " 'min_samples_split': 15,\n",
      " 'subsample': 0.9224934921181591}\n",
      "\n",
      "# training | log loss: 6.92%, AUC: 99.84%, accuracy: 97.83%\n",
      "# testing  | log loss: 12.51%, AUC: 98.82%, accuracy: 95.97%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "154 | Thu Sep 28 00:13:47 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 45\n",
      "{'learning_rate': 0.11736313453730748,\n",
      " 'max_depth': 8,\n",
      " 'max_features': None,\n",
      " 'min_samples_leaf': 9,\n",
      " 'min_samples_split': 13,\n",
      " 'subsample': 0.971854664482022}\n",
      "\n",
      "# training | log loss: 3.50%, AUC: 99.98%, accuracy: 99.33%\n",
      "# testing  | log loss: 10.15%, AUC: 99.03%, accuracy: 96.57%\n",
      "\n",
      "1 seconds.\n",
      "\n",
      "155 | Thu Sep 28 00:13:47 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 45\n",
      "{'learning_rate': 0.09830952699550209,\n",
      " 'max_depth': 10,\n",
      " 'max_features': None,\n",
      " 'min_samples_leaf': 9,\n",
      " 'min_samples_split': 6,\n",
      " 'subsample': 0.858192481778386}\n",
      "\n",
      "# training | log loss: 3.40%, AUC: 100.00%, accuracy: 99.73%\n",
      "# testing  | log loss: 10.04%, AUC: 99.04%, accuracy: 96.83%\n",
      "\n",
      "1 seconds.\n",
      "\n",
      "156 | Thu Sep 28 00:13:48 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 45\n",
      "{'learning_rate': 0.0906700945101033,\n",
      " 'max_depth': 9,\n",
      " 'max_features': None,\n",
      " 'min_samples_leaf': 7,\n",
      " 'min_samples_split': 17,\n",
      " 'subsample': 0.8542743881527751}\n",
      "\n",
      "# training | log loss: 3.99%, AUC: 99.99%, accuracy: 99.70%\n",
      "# testing  | log loss: 10.50%, AUC: 98.98%, accuracy: 96.60%\n",
      "\n",
      "1 seconds.\n",
      "\n",
      "157 | Thu Sep 28 00:13:49 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 45\n",
      "{'learning_rate': 0.11434410905390581,\n",
      " 'max_depth': 6,\n",
      " 'max_features': 'sqrt',\n",
      " 'min_samples_leaf': 10,\n",
      " 'min_samples_split': 19,\n",
      " 'subsample': 0.8756146563921536}\n",
      "\n",
      "# training | log loss: 9.75%, AUC: 99.73%, accuracy: 97.47%\n",
      "# testing  | log loss: 15.17%, AUC: 98.81%, accuracy: 95.56%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "*** 3.0 configurations x 27.0 iterations each\n",
      "\n",
      "158 | Thu Sep 28 00:13:49 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 135\n",
      "{'learning_rate': 0.17677822238193847,\n",
      " 'max_depth': 9,\n",
      " 'max_features': None,\n",
      " 'min_samples_leaf': 9,\n",
      " 'min_samples_split': 14,\n",
      " 'subsample': 0.8676310595202422}\n",
      "\n",
      "# training | log loss: 0.01%, AUC: 100.00%, accuracy: 100.00%\n",
      "# testing  | log loss: 11.21%, AUC: 98.96%, accuracy: 97.02%\n",
      "\n",
      "2 seconds.\n",
      "\n",
      "159 | Thu Sep 28 00:13:51 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 135\n",
      "{'learning_rate': 0.15762526041648256,\n",
      " 'max_depth': 7,\n",
      " 'max_features': None,\n",
      " 'min_samples_leaf': 8,\n",
      " 'min_samples_split': 20,\n",
      " 'subsample': 0.883756112950388}\n",
      "\n",
      "# training | log loss: 0.08%, AUC: 100.00%, accuracy: 100.00%\n",
      "# testing  | log loss: 10.21%, AUC: 99.00%, accuracy: 96.85%\n",
      "\n",
      "1 seconds.\n",
      "\n",
      "160 | Thu Sep 28 00:13:52 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 135\n",
      "{'learning_rate': 0.19647690528775552,\n",
      " 'max_depth': 8,\n",
      " 'max_features': 'log2',\n",
      " 'min_samples_leaf': 4,\n",
      " 'min_samples_split': 19,\n",
      " 'subsample': 0.802855622382238}\n",
      "\n",
      "# training | log loss: 0.05%, AUC: 100.00%, accuracy: 100.00%\n",
      "# testing  | log loss: 10.81%, AUC: 98.96%, accuracy: 96.91%\n",
      "\n",
      "1 seconds.\n",
      "\n",
      "*** 1.0 configurations x 81.0 iterations each\n",
      "\n",
      "161 | Thu Sep 28 00:13:53 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 405\n",
      "{'learning_rate': 0.15762526041648256,\n",
      " 'max_depth': 7,\n",
      " 'max_features': None,\n",
      " 'min_samples_leaf': 8,\n",
      " 'min_samples_split': 20,\n",
      " 'subsample': 0.883756112950388}\n",
      "\n",
      "# training | log loss: 0.01%, AUC: 100.00%, accuracy: 100.00%\n",
      "# testing  | log loss: 11.77%, AUC: 98.93%, accuracy: 96.92%\n",
      "\n",
      "2 seconds.\n",
      "\n",
      "*** 9 configurations x 9.0 iterations each\n",
      "\n",
      "162 | Thu Sep 28 00:13:55 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 45\n",
      "{'learning_rate': 0.18928467948833075,\n",
      " 'max_depth': 9,\n",
      " 'max_features': 'log2',\n",
      " 'min_samples_leaf': 4,\n",
      " 'min_samples_split': 9,\n",
      " 'subsample': 0.8796949603141723}\n",
      "\n",
      "# training | log loss: 1.38%, AUC: 100.00%, accuracy: 99.93%\n",
      "# testing  | log loss: 8.92%, AUC: 99.02%, accuracy: 97.02%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "163 | Thu Sep 28 00:13:56 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 45\n",
      "{'learning_rate': 0.1167735094382803,\n",
      " 'max_depth': 7,\n",
      " 'max_features': 'sqrt',\n",
      " 'min_samples_leaf': 5,\n",
      " 'min_samples_split': 17,\n",
      " 'subsample': 0.9074385955349866}\n",
      "\n",
      "# training | log loss: 6.62%, AUC: 99.90%, accuracy: 98.60%\n",
      "# testing  | log loss: 12.72%, AUC: 98.83%, accuracy: 96.34%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "164 | Thu Sep 28 00:13:56 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 45\n",
      "{'learning_rate': 0.16866148651470175,\n",
      " 'max_depth': 6,\n",
      " 'max_features': 'sqrt',\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 9,\n",
      " 'subsample': 0.8485221073925377}\n",
      "\n",
      "# training | log loss: 5.01%, AUC: 99.96%, accuracy: 99.07%\n",
      "# testing  | log loss: 11.87%, AUC: 98.96%, accuracy: 96.34%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "165 | Thu Sep 28 00:13:56 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 45\n",
      "{'learning_rate': 0.02228114160132352,\n",
      " 'max_depth': 7,\n",
      " 'max_features': 'log2',\n",
      " 'min_samples_leaf': 4,\n",
      " 'min_samples_split': 19,\n",
      " 'subsample': 0.8665645408815911}\n",
      "\n",
      "# training | log loss: 29.40%, AUC: 99.50%, accuracy: 96.17%\n",
      "# testing  | log loss: 32.83%, AUC: 98.57%, accuracy: 96.48%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "166 | Thu Sep 28 00:13:57 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 45\n",
      "{'learning_rate': 0.15987522816264135,\n",
      " 'max_depth': 9,\n",
      " 'max_features': 'sqrt',\n",
      " 'min_samples_leaf': 3,\n",
      " 'min_samples_split': 10,\n",
      " 'subsample': 0.8720633976918732}\n",
      "\n",
      "# training | log loss: 1.92%, AUC: 100.00%, accuracy: 99.83%\n",
      "# testing  | log loss: 9.42%, AUC: 99.06%, accuracy: 96.87%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "167 | Thu Sep 28 00:13:57 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 45\n",
      "{'learning_rate': 0.03359899578597631,\n",
      " 'max_depth': 6,\n",
      " 'max_features': 'log2',\n",
      " 'min_samples_leaf': 10,\n",
      " 'min_samples_split': 13,\n",
      " 'subsample': 0.8575833225251347}\n",
      "\n",
      "# training | log loss: 24.49%, AUC: 99.14%, accuracy: 95.43%\n",
      "# testing  | log loss: 27.62%, AUC: 98.32%, accuracy: 95.23%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "168 | Thu Sep 28 00:13:57 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 45\n",
      "{'learning_rate': 0.16132131350735907,\n",
      " 'max_depth': 3,\n",
      " 'max_features': 'sqrt',\n",
      " 'min_samples_leaf': 10,\n",
      " 'min_samples_split': 15,\n",
      " 'subsample': 0.9769712773105282}\n",
      "\n",
      "# training | log loss: 15.63%, AUC: 98.99%, accuracy: 95.20%\n",
      "# testing  | log loss: 20.53%, AUC: 98.16%, accuracy: 93.08%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "169 | Thu Sep 28 00:13:57 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 45\n",
      "{'learning_rate': 0.1479825290638138,\n",
      " 'max_depth': 5,\n",
      " 'max_features': 'sqrt',\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 11,\n",
      " 'subsample': 0.8659158155020963}\n",
      "\n",
      "# training | log loss: 9.16%, AUC: 99.72%, accuracy: 97.80%\n",
      "# testing  | log loss: 15.35%, AUC: 98.72%, accuracy: 95.34%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "170 | Thu Sep 28 00:13:57 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 45\n",
      "{'learning_rate': 0.13904925134254617,\n",
      " 'max_depth': 2,\n",
      " 'max_features': 'log2',\n",
      " 'min_samples_leaf': 2,\n",
      " 'min_samples_split': 6,\n",
      " 'subsample': 0.8388422068665762}\n",
      "\n",
      "# training | log loss: 22.18%, AUC: 98.25%, accuracy: 94.00%\n",
      "# testing  | log loss: 27.85%, AUC: 97.22%, accuracy: 91.17%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "*** 3.0 configurations x 27.0 iterations each\n",
      "\n",
      "171 | Thu Sep 28 00:13:58 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 135\n",
      "{'learning_rate': 0.18928467948833075,\n",
      " 'max_depth': 9,\n",
      " 'max_features': 'log2',\n",
      " 'min_samples_leaf': 4,\n",
      " 'min_samples_split': 9,\n",
      " 'subsample': 0.8796949603141723}\n",
      "\n",
      "# training | log loss: 0.01%, AUC: 100.00%, accuracy: 100.00%\n",
      "# testing  | log loss: 10.69%, AUC: 99.02%, accuracy: 97.27%\n",
      "\n",
      "1 seconds.\n",
      "\n",
      "172 | Thu Sep 28 00:13:59 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 135\n",
      "{'learning_rate': 0.15987522816264135,\n",
      " 'max_depth': 9,\n",
      " 'max_features': 'sqrt',\n",
      " 'min_samples_leaf': 3,\n",
      " 'min_samples_split': 10,\n",
      " 'subsample': 0.8720633976918732}\n",
      "\n",
      "# training | log loss: 0.02%, AUC: 100.00%, accuracy: 100.00%\n",
      "# testing  | log loss: 10.15%, AUC: 99.00%, accuracy: 97.23%\n",
      "\n",
      "1 seconds.\n",
      "\n",
      "173 | Thu Sep 28 00:14:00 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 135\n",
      "{'learning_rate': 0.16866148651470175,\n",
      " 'max_depth': 6,\n",
      " 'max_features': 'sqrt',\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 9,\n",
      " 'subsample': 0.8485221073925377}\n",
      "\n",
      "# training | log loss: 0.51%, AUC: 100.00%, accuracy: 100.00%\n",
      "# testing  | log loss: 10.14%, AUC: 98.93%, accuracy: 96.63%\n",
      "\n",
      "1 seconds.\n",
      "\n",
      "*** 1.0 configurations x 81.0 iterations each\n",
      "\n",
      "174 | Thu Sep 28 00:14:01 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 405\n",
      "{'learning_rate': 0.16866148651470175,\n",
      " 'max_depth': 6,\n",
      " 'max_features': 'sqrt',\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 9,\n",
      " 'subsample': 0.8485221073925377}\n",
      "\n",
      "# training | log loss: 0.01%, AUC: 100.00%, accuracy: 100.00%\n",
      "# testing  | log loss: 13.80%, AUC: 98.93%, accuracy: 96.71%\n",
      "\n",
      "2 seconds.\n",
      "\n",
      "*** 6 configurations x 27.0 iterations each\n",
      "\n",
      "175 | Thu Sep 28 00:14:03 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 135\n",
      "{'learning_rate': 0.13112203077076706,\n",
      " 'max_depth': 7,\n",
      " 'max_features': None,\n",
      " 'min_samples_leaf': 6,\n",
      " 'min_samples_split': 14,\n",
      " 'subsample': 0.9822861812148368}\n",
      "\n",
      "# training | log loss: 0.17%, AUC: 100.00%, accuracy: 100.00%\n",
      "# testing  | log loss: 9.53%, AUC: 99.01%, accuracy: 96.88%\n",
      "\n",
      "1 seconds.\n",
      "\n",
      "176 | Thu Sep 28 00:14:04 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 135\n",
      "{'learning_rate': 0.036425694425792535,\n",
      " 'max_depth': 5,\n",
      " 'max_features': 'log2',\n",
      " 'min_samples_leaf': 7,\n",
      " 'min_samples_split': 9,\n",
      " 'subsample': 0.8028770616754207}\n",
      "\n",
      "# training | log loss: 11.69%, AUC: 99.52%, accuracy: 96.63%\n",
      "# testing  | log loss: 15.99%, AUC: 98.68%, accuracy: 95.78%\n",
      "\n",
      "1 seconds.\n",
      "\n",
      "177 | Thu Sep 28 00:14:05 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 135\n",
      "{'learning_rate': 0.12135318704215547,\n",
      " 'max_depth': 3,\n",
      " 'max_features': 'log2',\n",
      " 'min_samples_leaf': 8,\n",
      " 'min_samples_split': 14,\n",
      " 'subsample': 0.9403485586176605}\n",
      "\n",
      "# training | log loss: 9.04%, AUC: 99.64%, accuracy: 97.13%\n",
      "# testing  | log loss: 14.66%, AUC: 98.75%, accuracy: 95.06%\n",
      "\n",
      "0 seconds.\n",
      "\n",
      "178 | Thu Sep 28 00:14:05 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 135\n",
      "{'learning_rate': 0.17521765934274502,\n",
      " 'max_depth': 9,\n",
      " 'max_features': None,\n",
      " 'min_samples_leaf': 6,\n",
      " 'min_samples_split': 15,\n",
      " 'subsample': 0.8221303739564697}\n",
      "\n",
      "# training | log loss: 0.01%, AUC: 100.00%, accuracy: 100.00%\n",
      "# testing  | log loss: 10.79%, AUC: 98.99%, accuracy: 97.21%\n",
      "\n",
      "2 seconds.\n",
      "\n",
      "179 | Thu Sep 28 00:14:07 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 135\n",
      "{'learning_rate': 0.19289004288231898,\n",
      " 'max_depth': 8,\n",
      " 'max_features': 'log2',\n",
      " 'min_samples_leaf': 5,\n",
      " 'min_samples_split': 11,\n",
      " 'subsample': 0.8876144971852449}\n",
      "\n",
      "# training | log loss: 0.04%, AUC: 100.00%, accuracy: 100.00%\n",
      "# testing  | log loss: 10.58%, AUC: 98.99%, accuracy: 97.02%\n",
      "\n",
      "1 seconds.\n",
      "\n",
      "180 | Thu Sep 28 00:14:08 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 135\n",
      "{'learning_rate': 0.10339224895303195,\n",
      " 'max_depth': 5,\n",
      " 'max_features': 'log2',\n",
      " 'min_samples_leaf': 5,\n",
      " 'min_samples_split': 8,\n",
      " 'subsample': 0.8904763290551876}\n",
      "\n",
      "# training | log loss: 4.34%, AUC: 99.96%, accuracy: 98.97%\n",
      "# testing  | log loss: 12.15%, AUC: 98.98%, accuracy: 95.88%\n",
      "\n",
      "1 seconds.\n",
      "\n",
      "*** 2.0 configurations x 81.0 iterations each\n",
      "\n",
      "181 | Thu Sep 28 00:14:08 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 405\n",
      "{'learning_rate': 0.13112203077076706,\n",
      " 'max_depth': 7,\n",
      " 'max_features': None,\n",
      " 'min_samples_leaf': 6,\n",
      " 'min_samples_split': 14,\n",
      " 'subsample': 0.9822861812148368}\n",
      "\n",
      "# training | log loss: 0.01%, AUC: 100.00%, accuracy: 100.00%\n",
      "# testing  | log loss: 12.08%, AUC: 98.94%, accuracy: 96.79%\n",
      "\n",
      "2 seconds.\n",
      "\n",
      "182 | Thu Sep 28 00:14:11 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 405\n",
      "{'learning_rate': 0.19289004288231898,\n",
      " 'max_depth': 8,\n",
      " 'max_features': 'log2',\n",
      " 'min_samples_leaf': 5,\n",
      " 'min_samples_split': 11,\n",
      " 'subsample': 0.8876144971852449}\n",
      "\n",
      "# training | log loss: 0.01%, AUC: 100.00%, accuracy: 100.00%\n",
      "# testing  | log loss: 12.19%, AUC: 98.91%, accuracy: 96.98%\n",
      "\n",
      "1 seconds.\n",
      "\n",
      "*** 5 configurations x 81.0 iterations each\n",
      "\n",
      "183 | Thu Sep 28 00:14:12 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 405\n",
      "{'learning_rate': 0.1594420401866271,\n",
      " 'max_depth': 8,\n",
      " 'max_features': 'sqrt',\n",
      " 'min_samples_leaf': 6,\n",
      " 'min_samples_split': 13,\n",
      " 'subsample': 0.8572243765786713}\n",
      "\n",
      "# training | log loss: 0.01%, AUC: 100.00%, accuracy: 100.00%\n",
      "# testing  | log loss: 12.00%, AUC: 98.94%, accuracy: 96.93%\n",
      "\n",
      "2 seconds.\n",
      "\n",
      "184 | Thu Sep 28 00:14:14 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 405\n",
      "{'learning_rate': 0.11238230171823492,\n",
      " 'max_depth': 7,\n",
      " 'max_features': 'sqrt',\n",
      " 'min_samples_leaf': 7,\n",
      " 'min_samples_split': 2,\n",
      " 'subsample': 0.9779662118961386}\n",
      "\n",
      "# training | log loss: 0.01%, AUC: 100.00%, accuracy: 100.00%\n",
      "# testing  | log loss: 12.63%, AUC: 98.96%, accuracy: 96.80%\n",
      "\n",
      "2 seconds.\n",
      "\n",
      "185 | Thu Sep 28 00:14:16 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 405\n",
      "{'learning_rate': 0.12260130852283373,\n",
      " 'max_depth': 2,\n",
      " 'max_features': 'log2',\n",
      " 'min_samples_leaf': 5,\n",
      " 'min_samples_split': 13,\n",
      " 'subsample': 0.9554721442163021}\n",
      "\n",
      "# training | log loss: 7.70%, AUC: 99.73%, accuracy: 97.50%\n",
      "# testing  | log loss: 13.96%, AUC: 98.80%, accuracy: 95.26%\n",
      "\n",
      "1 seconds.\n",
      "\n",
      "186 | Thu Sep 28 00:14:17 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 405\n",
      "{'learning_rate': 0.1862877504155004,\n",
      " 'max_depth': 7,\n",
      " 'max_features': None,\n",
      " 'min_samples_leaf': 4,\n",
      " 'min_samples_split': 17,\n",
      " 'subsample': 0.9266528307856066}\n",
      "\n",
      "# training | log loss: 0.01%, AUC: 100.00%, accuracy: 100.00%\n",
      "# testing  | log loss: 12.04%, AUC: 98.95%, accuracy: 96.84%\n",
      "\n",
      "2 seconds.\n",
      "\n",
      "187 | Thu Sep 28 00:14:19 2017 | lowest loss so far: 0.0861 (run 111)\n",
      "\n",
      "n_estimators: 405\n",
      "{'learning_rate': 0.09506206039577023,\n",
      " 'max_depth': 5,\n",
      " 'max_features': 'log2',\n",
      " 'min_samples_leaf': 4,\n",
      " 'min_samples_split': 10,\n",
      " 'subsample': 0.8948388643386312}\n",
      "\n",
      "# training | log loss: 0.40%, AUC: 100.00%, accuracy: 100.00%\n",
      "# testing  | log loss: 10.89%, AUC: 98.93%, accuracy: 96.42%\n",
      "\n",
      "2 seconds.\n"
     ]
    }
   ],
   "source": [
    "hb = Hyperband(get_params, try_params)\n",
    "results = hb.run(data=data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tic-tac-toe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/tictac.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = data.rename(columns={'1.5':'target'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_prop = 0.3\n",
    "n_0 = data[data['target'] == 0].shape[0]\n",
    "n_1 = data[data['target'] == 1].shape[0]\n",
    "train_0 = random.sample(range(n_0), int(n_0*train_prop))\n",
    "train_1 = random.sample(range(n_1), int(n_1*train_prop))\n",
    "test_0 = [i for i in range(n_0) if i not in train_0]\n",
    "test_1 = [i for i in range(n_1) if i not in train_1]\n",
    "train = pd.concat((data[data['target'] == 0].iloc[train_0,:], data[data['target'] == 1].iloc[train_1,:]))\n",
    "test = pd.concat((data[data['target'] == 0].iloc[test_0,:], data[data['target'] == 1].iloc[test_1,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dict = {\n",
    "    'x_train': train.drop('target', axis=1),\n",
    "    'y_train': train['target'],\n",
    "    'x_test': test.drop('target', axis=1),\n",
    "    'y_test': test['target']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** 81 configurations x 1.0 iterations each\n",
      "\n",
      "1 | Thu Oct 26 04:19:56 2017 | lowest loss so far: inf (run -1)\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'dict' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-3a6ebeae34e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mhb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHyperband\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtry_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/javier.mas/hyperband/hyperband/hyperband.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data, skip_last, dry_run)\u001b[0m\n\u001b[1;32m     65\u001b[0m                                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \t\t\t\t\t\tresult = self.try_params( n_iterations=n_iterations, \n\u001b[0;32m---> 67\u001b[0;31m                                                                           \u001b[0mt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \t\t\t\t\t\t\t\t\t  data=data )\t\t# <---\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/javier.mas/hyperband/hyperband/defs/xgb.py\u001b[0m in \u001b[0;36mtry_params\u001b[0;34m(data, n_iterations, params, get_predictions)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtry_params\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mn_iterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mpprint\u001b[0m \u001b[0mn_iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mn_estimators\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;31m#n_estimators = int( round( n_iterations * trees_per_iteration ))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m\"n_estimators:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'dict' and 'int'"
     ]
    }
   ],
   "source": [
    "hb = Hyperband(get_params, try_params)\n",
    "results = hb.run(data=data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
